{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DINO: Self-Supervised Vision Learning","text":"<p>Learn powerful visual representations without labels using DINO (Self-DIstillation with NO labels). This is a clean, production-ready PyTorch implementation.</p>"},{"location":"#what-is-dino","title":"What is DINO?","text":"<p>DINO teaches a neural network to recognize images without any labels. It works by:</p> <ol> <li>Taking one image and creating multiple different views (crops, color changes, etc.)</li> <li>A \"student\" network tries to predict what a \"teacher\" network sees</li> <li>The teacher slowly learns from the student's improvements</li> </ol> <p>Result: The network learns powerful visual features that work for many downstream tasks (classification, detection, segmentation).</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Easy to use: Train with a single command</li> <li>Production ready: Clean code structure, proper logging, checkpointing</li> <li>Configurable: YAML configs for all hyperparameters</li> <li>Extensible: Easy to add new datasets or backbones</li> <li>Bug-free: Fixed the negative loss bug from the original notebook</li> <li>Multiple datasets: ImageNette and ImageNet100 included</li> <li>Training history: Track and visualize loss, learning rate, and momentum</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>:material-download:{ .lg .middle } Getting Started</p> <p>Install DINO and train your first model in minutes</p> <p>:octicons-arrow-right-24: Installation</p> </li> <li> <p>:material-book-open-variant:{ .lg .middle } Guides</p> <p>Learn about data pipelines, models, training, and more</p> <p>:octicons-arrow-right-24: Data Pipeline</p> </li> <li> <p>:material-code-braces:{ .lg .middle } API Reference</p> <p>Complete API documentation generated from source code</p> <p>:octicons-arrow-right-24: API Reference</p> </li> <li> <p>:material-wrench:{ .lg .middle } Troubleshooting</p> <p>Solutions to common problems</p> <p>:octicons-arrow-right-24: Troubleshooting</p> </li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>DinoImpl/\n\u251c\u2500\u2500 src/dino/              # Main package\n\u2502   \u251c\u2500\u2500 config/           # Configuration management\n\u2502   \u251c\u2500\u2500 data/             # Data loading &amp; augmentation\n\u2502   \u251c\u2500\u2500 models/           # Neural network models\n\u2502   \u2502   \u2514\u2500\u2500 backbone/     # Backbone architectures (ResNet, ViT)\n\u2502   \u251c\u2500\u2500 loss/             # Loss functions\n\u2502   \u251c\u2500\u2500 training/         # Training logic\n\u2502   \u251c\u2500\u2500 evaluation/       # Evaluation utilities\n\u2502   \u2514\u2500\u2500 utils/            # Utilities (checkpointing, logging, history, etc.)\n\u2502\n\u251c\u2500\u2500 configs/              # YAML configuration files\n\u251c\u2500\u2500 scripts/              # Executable scripts\n\u251c\u2500\u2500 checkpoints/          # Saved model checkpoints (auto-created)\n\u251c\u2500\u2500 logs/                 # Training logs (auto-created)\n\u2514\u2500\u2500 data/                 # Datasets (auto-downloaded)\n</code></pre>"},{"location":"#how-it-works-simple-explanation","title":"How It Works (Simple Explanation)","text":"<ol> <li>Multi-Crop Augmentation: Takes one image, creates 8 different views (2 large, 6 small)</li> <li>Student Network: Looks at all 8 views and makes predictions</li> <li>Teacher Network: Only looks at the 2 large views</li> <li>Loss: Student tries to match teacher's predictions</li> <li>EMA Update: Teacher slowly copies student's weights</li> <li>Centering: Prevents the model from always predicting the same thing</li> </ol> <p>Why this works: The student has to learn robust features that work across different views, scales, and augmentations.</p>"},{"location":"#learn-more","title":"Learn More","text":"<ul> <li>Original Paper: DINO: Emerging Properties in Self-Supervised Vision Transformers</li> <li>Official Implementation: facebookresearch/dino</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Solutions to common problems when using DINO.</p>"},{"location":"troubleshooting/#training-issues","title":"Training Issues","text":""},{"location":"troubleshooting/#loss-is-nan-or-not-decreasing","title":"Loss is NaN or Not Decreasing","text":"<p>Symptoms: Loss shows <code>nan</code> or stays flat</p> <p>Solutions:</p> <ol> <li> <p>Reduce learning rate:    <pre><code>optimizer:\n  lr: 0.0001  # Try lower values\n</code></pre></p> </li> <li> <p>Check batch size (need sufficient examples):    <pre><code>data:\n  batch_size: 32  # Minimum recommended\n</code></pre></p> </li> <li> <p>Verify data pipeline:    <pre><code>for views, labels in train_loader:\n    print(f\"Views: {len(views)}, Shape: {views[0].shape}\")\n    print(f\"Min: {views[0].min()}, Max: {views[0].max()}\")\n    break\n</code></pre></p> </li> <li> <p>Check for data issues:</p> </li> <li>Corrupted images</li> <li>Wrong normalization values</li> <li>Empty batches</li> </ol>"},{"location":"troubleshooting/#loss-is-negative","title":"Loss is Negative","text":"<p>This should not happen with this implementation. The original notebook had a bug that caused negative loss - it's fixed here.</p> <p>If you see negative loss:</p> <ol> <li> <p>Verify you're using the correct loss class:    <pre><code>from dino.loss import DinoLoss  # Use this\n</code></pre></p> </li> <li> <p>Check that outputs are correctly chunked:    <pre><code># Correct: 8 student views, 2 teacher views\nprint(f\"Student output shape: {student_output.shape}\")\nprint(f\"Teacher output shape: {teacher_output.shape}\")\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>Symptoms: <code>CUDA out of memory</code> error</p> <p>Solutions:</p> <ol> <li> <p>Reduce batch size:    <pre><code>data:\n  batch_size: 16  # or 8\n</code></pre></p> </li> <li> <p>Reduce local crops:    <pre><code>augmentation:\n  num_local_views: 4  # Instead of 6\n</code></pre></p> </li> <li> <p>Use smaller backbone:    <pre><code>model:\n  backbone: resnet18  # Instead of resnet50\n</code></pre></p> </li> <li> <p>Enable gradient checkpointing (advanced):    <pre><code>from torch.utils.checkpoint import checkpoint\n# Wrap forward passes\n</code></pre></p> </li> <li> <p>Use mixed precision:    <pre><code>from torch.cuda.amp import autocast\nwith autocast():\n    outputs = model(inputs)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#out-of-memory-with-vit-backbone","title":"Out of Memory with ViT Backbone","text":"<p>ViT backbones require more memory than ResNet.</p> <p>Solutions:</p> <ol> <li> <p>Reduce batch size significantly:    <pre><code>data:\n  batch_size: 8  # ViT needs smaller batches\n</code></pre></p> </li> <li> <p>Use smaller ViT variant:    <pre><code>model:\n  backbone: dino_vits16  # Instead of dino_vitb16\n</code></pre></p> </li> </ol> <p>Memory requirements:    - <code>dino_vits8/vits16</code>: ~8GB with batch_size=16    - <code>dino_vitb8/vitb16</code>: ~16GB with batch_size=16</p>"},{"location":"troubleshooting/#slow-training","title":"Slow Training","text":"<p>Symptoms: Training is unexpectedly slow, GPU utilization low</p> <p>Solutions:</p> <ol> <li> <p>Increase data loading workers:    <pre><code>data:\n  num_workers: 8  # Match CPU cores\n</code></pre></p> </li> <li> <p>Enable pin memory:    <pre><code>data:\n  pin_memory: true\n</code></pre></p> </li> <li> <p>Check GPU utilization:    <pre><code>nvidia-smi -l 1\n</code></pre></p> </li> <li>If GPU util &lt; 50%: Data loading bottleneck</li> <li> <p>If GPU util &gt; 90%: Normal</p> </li> <li> <p>Use SSD for data: Spinning disks are slow for random access</p> </li> </ol>"},{"location":"troubleshooting/#data-issues","title":"Data Issues","text":""},{"location":"troubleshooting/#dataset-not-found","title":"Dataset Not Found","text":"<p>Error: <code>FileNotFoundError</code> or <code>Dataset not found</code></p> <p>Solutions:</p> <ol> <li> <p>Check data path:    <pre><code>data:\n  data_path: ./data  # Verify this path exists\n</code></pre></p> </li> <li> <p>For ImageNette: It should auto-download. Check internet connection.</p> </li> <li> <p>For ImageNet100: Manual download required:    <pre><code>kaggle datasets download -d ambityga/imagenet100\nunzip imagenet100.zip -d ./data/imagenet100\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#corrupted-images","title":"Corrupted Images","text":"<p>Symptoms: <code>PIL.UnidentifiedImageError</code> or similar</p> <p>Solutions:</p> <ol> <li> <p>Find corrupted files:    <pre><code>from PIL import Image\nimport os\n\nfor root, dirs, files in os.walk('./data'):\n    for f in files:\n        if f.endswith(('.jpg', '.png', '.jpeg')):\n            try:\n                Image.open(os.path.join(root, f)).verify()\n            except:\n                print(f\"Corrupted: {os.path.join(root, f)}\")\n</code></pre></p> </li> <li> <p>Remove or replace corrupted files</p> </li> </ol>"},{"location":"troubleshooting/#checkpoint-issues","title":"Checkpoint Issues","text":""},{"location":"troubleshooting/#cannot-load-checkpoint","title":"Cannot Load Checkpoint","text":"<p>Error: <code>KeyError</code> or shape mismatch when loading</p> <p>Causes:</p> <ol> <li>Architecture mismatch: Checkpoint from different model configuration</li> <li>Missing keys: Checkpoint from older code version</li> </ol> <p>Solutions:</p> <ol> <li> <p>Use strict=False (may lose some weights):    <pre><code>model.load_state_dict(checkpoint['state_dict'], strict=False)\n</code></pre></p> </li> <li> <p>Verify config matches:    <pre><code>checkpoint = torch.load('checkpoint.pth')\nprint(checkpoint['config'])  # Compare with current config\n</code></pre></p> </li> <li> <p>Retrain from scratch if architecture changed significantly</p> </li> </ol>"},{"location":"troubleshooting/#checkpoint-too-large","title":"Checkpoint Too Large","text":"<p>Symptoms: Checkpoint files are very large (&gt;1GB)</p> <p>Cause: Saving unnecessary data (optimizer state, etc.)</p> <p>Solution: For inference, save only model weights: <pre><code>torch.save({\n    'model_state_dict': model.state_dict(),\n}, 'model_only.pth')\n</code></pre></p>"},{"location":"troubleshooting/#configuration-issues","title":"Configuration Issues","text":""},{"location":"troubleshooting/#yaml-parsing-errors","title":"YAML Parsing Errors","text":"<p>Error: <code>yaml.scanner.ScannerError</code></p> <p>Common causes:</p> <ol> <li>Indentation errors: YAML requires consistent indentation</li> <li>Special characters: Escape colons in strings</li> </ol> <p>Solutions: <pre><code># Wrong\ndata:\ndataset: imagenette  # Missing indentation\n\n# Correct\ndata:\n  dataset: imagenette\n</code></pre></p>"},{"location":"troubleshooting/#type-errors-in-config","title":"Type Errors in Config","text":"<p>Error: <code>TypeError</code> when loading config</p> <p>Solution: Check types match dataclass definitions: <pre><code># Wrong\ndata:\n  batch_size: \"32\"  # String, should be int\n\n# Correct\ndata:\n  batch_size: 32\n</code></pre></p>"},{"location":"troubleshooting/#gpu-issues","title":"GPU Issues","text":""},{"location":"troubleshooting/#cuda-not-available","title":"CUDA Not Available","text":"<pre><code>import torch\nprint(torch.cuda.is_available())  # Should be True\n</code></pre> <p>Solutions:</p> <ol> <li>Reinstall PyTorch with CUDA: pytorch.org</li> <li>Check NVIDIA drivers: <code>nvidia-smi</code></li> <li>Verify CUDA version compatibility</li> </ol>"},{"location":"troubleshooting/#wrong-gpu-selected","title":"Wrong GPU Selected","text":"<p>Solution: Set GPU explicitly: <pre><code>CUDA_VISIBLE_DEVICES=0 python scripts/train.py\n</code></pre></p> <p>Or in Python: <pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n</code></pre></p>"},{"location":"troubleshooting/#getting-help","title":"Getting Help","text":"<p>If your issue isn't listed here:</p> <ol> <li>Check the logs: Look for error messages in <code>./logs/</code></li> <li>Enable debug mode: Set <code>log_level: DEBUG</code> in config</li> <li>Minimal reproduction: Create a minimal example that reproduces the issue</li> <li>Open an issue: Include error message, config, and environment info</li> </ol>"},{"location":"troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Installation - Setup guide</li> <li>Configuration - Config reference</li> <li>Performance - Optimization tips</li> </ul>"},{"location":"advanced/design-decisions/","title":"Design Decisions","text":"<p>This document explains the key architectural decisions in this DINO implementation.</p>"},{"location":"advanced/design-decisions/#design-philosophy","title":"Design Philosophy","text":"<p>This implementation follows modern Python best practices:</p> <ul> <li>Modular design: Each component has a single, well-defined responsibility</li> <li>Type safety: Type hints throughout for better IDE support and error catching</li> <li>Configuration over code: All hyperparameters in YAML, no magic numbers</li> <li>Separation of concerns: Data, models, training, and utilities are isolated</li> <li>Production ready: Proper logging, error handling, and checkpointing</li> </ul>"},{"location":"advanced/design-decisions/#key-decisions","title":"Key Decisions","text":""},{"location":"advanced/design-decisions/#1-why-dataclasses-for-configuration","title":"1. Why Dataclasses for Configuration?","text":"<p>Alternatives considered: - Dict-based configs (e.g., <code>config['data']['batch_size']</code>) - Argparse-only (CLI args) - Hydra framework</p> <p>Chosen: Dataclasses + YAML</p> <p>Reasons: - Type safety and IDE autocomplete - No runtime dependencies (Hydra is heavy) - Easy serialization with YAML - Hierarchical structure matches logical grouping - Simple to test and validate</p> <p>Example: <pre><code>@dataclass\nclass DataConfig:\n    dataset: str = 'imagenette'\n    batch_size: int = 32\n    num_workers: int = 4\n</code></pre></p>"},{"location":"advanced/design-decisions/#2-why-separate-backbone-and-projection","title":"2. Why Separate Backbone and Projection?","text":"<p>Alternative: Single monolithic model</p> <p>Chosen: Composition (DinoModel = Backbone + Projection)</p> <p>Reasons: - Backbone features useful for downstream tasks - Can replace backbone without changing projection logic - Can freeze backbone for linear evaluation - Clear separation of feature extraction vs. projection - Matches DINO paper architecture</p> <p>Implementation: <pre><code>class DinoModel(nn.Module):\n    def __init__(self, backbone, projection_head):\n        self.backbone = backbone\n        self.projection_head = projection_head\n\n    def forward(self, x, return_backbone_features=False):\n        features = self.backbone(x)\n        projections = self.projection_head(features)\n        if return_backbone_features:\n            return features, projections\n        return projections\n</code></pre></p>"},{"location":"advanced/design-decisions/#3-why-custom-collate-function","title":"3. Why Custom Collate Function?","text":"<p>Problem: Multi-crop transform returns list of tensors</p> <p>Default collate behavior: <pre><code>batch = [(views_list, label), ...]\ndefault_collate(batch)  # Fails! Can't stack lists\n</code></pre></p> <p>Solution: Custom collate transposes data structure <pre><code># Input: [[g1_img1, g2_img1, l1_img1, ...], [g1_img2, ...], ...]\n# Output: [[g1_img1, g1_img2, ...], [g2_img1, g2_img2, ...], ...]\n\ndef collate_multi_crop(batch):\n    views_lists = [item[0] for item in batch]\n    num_views = len(views_lists[0])\n    views_batch = [\n        torch.stack([views[i] for views in views_lists])\n        for i in range(num_views)\n    ]\n    labels = torch.tensor([item[1] for item in batch])\n    return views_batch, labels\n</code></pre></p>"},{"location":"advanced/design-decisions/#4-why-ema-instead-of-two-optimizers","title":"4. Why EMA Instead of Two Optimizers?","text":"<p>Alternative: Train two models with different optimizers</p> <p>Chosen: One student + EMA teacher</p> <p>Reasons: - Teacher needs to be stable (slow-changing) - EMA provides smooth, consistent updates - Prevents teacher from diverging - Computationally efficient (no backprop through teacher) - Matches DINO paper</p> <p>Implementation: <pre><code>@torch.no_grad()\ndef update_teacher_EMA(student, teacher, momentum=0.996):\n    for s_param, t_param in zip(student.parameters(), teacher.parameters()):\n        t_param.data = momentum * t_param.data + (1 - momentum) * s_param.data\n</code></pre></p>"},{"location":"advanced/design-decisions/#5-why-checkpoint-both-student-and-teacher","title":"5. Why Checkpoint Both Student and Teacher?","text":"<p>Alternative: Only save student (can reconstruct teacher with EMA)</p> <p>Chosen: Save both</p> <p>Reasons: - Teacher state is not deterministic from student alone - EMA accumulation history is important - Ensures exact resumption of training - Minimal storage overhead (weights are shared structure)</p>"},{"location":"advanced/design-decisions/#6-why-factory-methods-from_config","title":"6. Why Factory Methods (<code>from_config</code>)?","text":"<p>Alternative: Manual construction everywhere</p> <p>Chosen: Factory methods for all major components</p> <p>Benefits: - ~90 lines reduced to ~30 lines in train.py - Correct wiring: Dimensions automatically matched - Consistency: Same pattern across components - Testability: Factory methods can be unit tested</p> <p>Pattern: <pre><code># Factory method\nmodel = DinoModel.from_config(config)\n\n# vs. manual (still works)\nbackbone = get_backbone('resnet18')\nprojection = DinoProjectionHead(input_dim=512, output_dim=2048)\nmodel = DinoModel(backbone, projection)\n</code></pre></p>"},{"location":"advanced/design-decisions/#7-why-save-loss-center-in-checkpoints","title":"7. Why Save Loss Center in Checkpoints?","text":"<p>The DINO loss maintains a running center via EMA:</p> <pre><code>center = momentum * center + (1 - momentum) * teacher_outputs.mean()\n</code></pre> <p>Why save it? - Center accumulates over many iterations - Cannot be reconstructed from a single batch - Critical for stable loss computation - Required for correct training resumption</p>"},{"location":"advanced/design-decisions/#8-why-torchvision-for-resnet-huggingface-for-vit","title":"8. Why torchvision for ResNet, HuggingFace for ViT?","text":"<p>ResNet via torchvision: - Standard, well-tested implementations - Easy to remove classifier head - Built into PyTorch ecosystem</p> <p>ViT via HuggingFace: - Pre-trained DINO ViT models available - Handles position encoding interpolation - Active community and updates</p> <p>Alternative considered: timm for both - Pro: Single dependency - Con: Different API patterns, less common for ViT</p>"},{"location":"advanced/design-decisions/#module-responsibilities","title":"Module Responsibilities","text":"Module Responsibility Key Classes <code>config</code> Configuration management <code>DinoConfig</code>, <code>DataConfig</code> <code>data</code> Data pipeline <code>DINOTransform</code>, <code>get_dataset</code> <code>models</code> Neural networks <code>DinoModel</code>, <code>get_backbone</code> <code>loss</code> Loss computation <code>DinoLoss</code> <code>training</code> Training loop <code>DinoTrainer</code> <code>utils</code> Cross-cutting concerns <code>save_checkpoint</code>, <code>History</code>"},{"location":"advanced/design-decisions/#technology-stack","title":"Technology Stack","text":"<ul> <li>PyTorch 2.0+: Core deep learning framework</li> <li>torchvision: Pre-trained models and transforms</li> <li>HuggingFace transformers: ViT backbones</li> <li>uv: Fast, modern Python package manager</li> <li>YAML: Human-readable configuration format</li> <li>Python dataclasses: Type-safe configuration objects</li> <li>tqdm: Progress bars for training loops</li> </ul>"},{"location":"advanced/design-decisions/#trade-offs-made","title":"Trade-offs Made","text":"Decision Benefit Cost Separate backbone/projection Flexibility, clarity Slightly more code Dataclasses for config Type safety, IDE support Learning curve Custom collate Correct multi-crop handling Extra code Save teacher weights Exact resumption Larger checkpoints Factory methods Less boilerplate Additional abstraction"},{"location":"advanced/design-decisions/#what-we-dont-do","title":"What We Don't Do","text":"<ul> <li>No backward compatibility with old checkpoints: Simpler code</li> <li>No automatic hyperparameter search: Out of scope</li> <li>No distributed training built-in: Use PyTorch's DDP directly</li> <li>No early stopping: Users can implement if needed</li> </ul>"},{"location":"advanced/design-decisions/#see-also","title":"See Also","text":"<ul> <li>Extending DINO - Adding custom components</li> <li>Performance - Optimization guide</li> <li>Configuration - Full config reference</li> </ul>"},{"location":"advanced/extending/","title":"Extending DINO","text":"<p>Guide to adding custom backbones, datasets, and evaluation metrics.</p>"},{"location":"advanced/extending/#adding-a-new-backbone","title":"Adding a New Backbone","text":"<p>The codebase already includes two backbone implementations as examples:</p> <ol> <li>ResNet (<code>src/dino/models/backbone/resnet.py</code>): Uses torchvision</li> <li>DINO ViT (<code>src/dino/models/backbone/vit.py</code>): Uses HuggingFace transformers</li> </ol>"},{"location":"advanced/extending/#step-1-create-backbone-class","title":"Step 1: Create Backbone Class","text":"<p>Create a new file in <code>src/dino/models/backbone/</code>:</p> <pre><code># src/dino/models/backbone/efficientnet.py\nfrom .backbone import BackboneBase\nimport timm\n\nclass EfficientNetBackbone(BackboneBase):\n    \"\"\"EfficientNet backbone using timm.\"\"\"\n\n    VARIANTS = {\n        'efficientnet_b0': 1280,\n        'efficientnet_b1': 1280,\n        'efficientnet_b2': 1408,\n        'efficientnet_b3': 1536,\n        'efficientnet_b4': 1792,\n    }\n\n    def __init__(self, variant='efficientnet_b0', pretrained=False):\n        super().__init__()\n        if variant not in self.VARIANTS:\n            raise ValueError(f\"Unknown variant: {variant}\")\n\n        self.model = timm.create_model(\n            variant,\n            pretrained=pretrained,\n            num_classes=0  # Remove classifier\n        )\n        self._output_dim = self.VARIANTS[variant]\n\n    @property\n    def output_dim(self) -&gt; int:\n        return self._output_dim\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"advanced/extending/#step-2-update-factory-function","title":"Step 2: Update Factory Function","text":"<p>Edit <code>src/dino/models/backbone/backbone.py</code>:</p> <pre><code>def get_backbone(name: str, pretrained: bool = False) -&gt; BackboneBase:\n    \"\"\"Factory function for creating backbones.\"\"\"\n\n    if name.startswith('resnet'):\n        from .resnet import ResnetBackboneDino\n        return ResnetBackboneDino(name, pretrained)\n\n    elif name.startswith('dino_vit'):\n        from .vit import DinoBackbone\n        return DinoBackbone(name, pretrained)\n\n    elif name.startswith('efficientnet'):\n        from .efficientnet import EfficientNetBackbone\n        return EfficientNetBackbone(name, pretrained)\n\n    else:\n        raise ValueError(f\"Unknown backbone: {name}\")\n</code></pre>"},{"location":"advanced/extending/#step-3-export-in-__init__py","title":"Step 3: Export in <code>__init__.py</code>","text":"<p>Edit <code>src/dino/models/backbone/__init__.py</code>:</p> <pre><code>from .backbone import BackboneBase, get_backbone\nfrom .resnet import ResnetBackboneDino\nfrom .vit import DinoBackbone\nfrom .efficientnet import EfficientNetBackbone\n\n__all__ = [\n    'BackboneBase',\n    'get_backbone',\n    'ResnetBackboneDino',\n    'DinoBackbone',\n    'EfficientNetBackbone',\n]\n</code></pre>"},{"location":"advanced/extending/#step-4-use-in-config","title":"Step 4: Use in Config","text":"<pre><code>model:\n  backbone: efficientnet_b0\n  pretrained_backbone: true\n</code></pre>"},{"location":"advanced/extending/#adding-a-new-dataset","title":"Adding a New Dataset","text":""},{"location":"advanced/extending/#step-1-add-to-get_dataset","title":"Step 1: Add to <code>get_dataset()</code>","text":"<p>Edit <code>src/dino/data/datasets.py</code>:</p> <pre><code>def get_dataset(\n    name: str,\n    path: str,\n    transform,\n    split: str = 'train'\n) -&gt; Dataset:\n    \"\"\"Factory function for creating datasets.\"\"\"\n\n    if name == 'imagenette':\n        return Imagenette(root=path, split=split, transform=transform)\n\n    elif name == 'imagenet100':\n        return get_imagenet100_dataset(path, split, transform)\n\n    elif name == 'cifar10':\n        return torchvision.datasets.CIFAR10(\n            root=path,\n            train=(split == 'train'),\n            transform=transform,\n            download=True\n        )\n\n    elif name == 'custom_folder':\n        # ImageFolder for any folder of images\n        split_path = os.path.join(path, split)\n        return torchvision.datasets.ImageFolder(\n            root=split_path,\n            transform=transform\n        )\n\n    else:\n        raise ValueError(f\"Unknown dataset: {name}\")\n</code></pre>"},{"location":"advanced/extending/#step-2-create-config-file","title":"Step 2: Create Config File","text":"<p>Create <code>configs/cifar10.yaml</code>:</p> <pre><code>data:\n  dataset: cifar10\n  data_path: ./data\n  batch_size: 128\n\naugmentation:\n  global_crop_size: 32    # CIFAR images are 32x32\n  local_crop_size: 16\n  global_crop_scale: [0.5, 1.0]\n  local_crop_scale: [0.1, 0.5]\n\nmodel:\n  backbone: resnet18\n  projection_output_dim: 1024\n</code></pre>"},{"location":"advanced/extending/#step-3-run-training","title":"Step 3: Run Training","text":"<pre><code>python scripts/train.py --config configs/cifar10.yaml\n</code></pre>"},{"location":"advanced/extending/#adding-evaluation-metrics","title":"Adding Evaluation Metrics","text":""},{"location":"advanced/extending/#step-1-create-evaluator","title":"Step 1: Create Evaluator","text":"<p>Create <code>src/dino/evaluation/knn.py</code>:</p> <pre><code>import torch\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom typing import Tuple, Dict\n\nclass KNNEvaluator:\n    \"\"\"K-Nearest Neighbors evaluation for self-supervised features.\"\"\"\n\n    def __init__(self, model, k: int = 20, device: str = 'cuda'):\n        self.model = model\n        self.k = k\n        self.device = device\n\n    @torch.no_grad()\n    def extract_features(self, dataloader) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Extract features from a dataloader.\"\"\"\n        self.model.eval()\n        features_list = []\n        labels_list = []\n\n        for images, labels in dataloader:\n            images = images.to(self.device)\n\n            # Get backbone features (not projections)\n            features, _ = self.model(images, return_backbone_features=True)\n            features = features.cpu().numpy()\n\n            features_list.append(features)\n            labels_list.append(labels.numpy())\n\n        return np.concatenate(features_list), np.concatenate(labels_list)\n\n    def evaluate(\n        self,\n        train_loader,\n        val_loader\n    ) -&gt; Dict[str, float]:\n        \"\"\"Run KNN evaluation.\"\"\"\n        # Extract features\n        train_features, train_labels = self.extract_features(train_loader)\n        val_features, val_labels = self.extract_features(val_loader)\n\n        # Normalize features\n        train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n        val_features = val_features / np.linalg.norm(val_features, axis=1, keepdims=True)\n\n        # Fit KNN\n        knn = KNeighborsClassifier(n_neighbors=self.k, metric='cosine')\n        knn.fit(train_features, train_labels)\n\n        # Evaluate\n        accuracy = knn.score(val_features, val_labels)\n\n        return {\n            'knn_accuracy': accuracy,\n            'k': self.k\n        }\n</code></pre>"},{"location":"advanced/extending/#step-2-export-in-__init__py","title":"Step 2: Export in <code>__init__.py</code>","text":"<p>Edit <code>src/dino/evaluation/__init__.py</code>:</p> <pre><code>from .knn import KNNEvaluator\n\n__all__ = ['KNNEvaluator']\n</code></pre>"},{"location":"advanced/extending/#step-3-add-to-trainer-optional","title":"Step 3: Add to Trainer (Optional)","text":"<pre><code># In trainer.py\nclass DinoTrainer:\n    def __init__(self, ..., evaluator=None):\n        self.evaluator = evaluator\n\n    def validate_epoch(self, epoch):\n        if self.evaluator:\n            # Create eval dataloaders (without multi-crop)\n            eval_train_loader = create_eval_dataloader(self.train_loader.dataset)\n            eval_val_loader = create_eval_dataloader(self.val_loader.dataset)\n\n            metrics = self.evaluator.evaluate(eval_train_loader, eval_val_loader)\n            self.logger.info(f\"KNN Accuracy: {metrics['knn_accuracy']:.4f}\")\n            return metrics\n        return {}\n</code></pre>"},{"location":"advanced/extending/#step-4-use-evaluator","title":"Step 4: Use Evaluator","text":"<pre><code>from dino.evaluation import KNNEvaluator\n\n# After training\nevaluator = KNNEvaluator(model=teacher, k=20, device='cuda')\nmetrics = evaluator.evaluate(train_loader, val_loader)\nprint(f\"KNN Accuracy: {metrics['knn_accuracy']:.2%}\")\n</code></pre>"},{"location":"advanced/extending/#adding-custom-augmentations","title":"Adding Custom Augmentations","text":""},{"location":"advanced/extending/#step-1-create-custom-transform","title":"Step 1: Create Custom Transform","text":"<pre><code># src/dino/data/custom_transforms.py\nimport torchvision.transforms as T\n\nclass CustomAugmentation:\n    \"\"\"Custom augmentation for specific dataset.\"\"\"\n\n    def __init__(self, p: float = 0.5):\n        self.transform = T.Compose([\n            T.RandomApply([\n                T.GaussianBlur(kernel_size=23)\n            ], p=p),\n            # Add more augmentations\n        ])\n\n    def __call__(self, img):\n        return self.transform(img)\n</code></pre>"},{"location":"advanced/extending/#step-2-integrate-with-dinotransform","title":"Step 2: Integrate with DINOTransform","text":"<pre><code># In DINOTransform\nclass DINOTransform:\n    def __init__(self, ..., custom_aug=None):\n        self.custom_aug = custom_aug\n\n    def __call__(self, img):\n        if self.custom_aug:\n            img = self.custom_aug(img)\n        # Continue with standard transforms\n        ...\n</code></pre>"},{"location":"advanced/extending/#adding-custom-loss-functions","title":"Adding Custom Loss Functions","text":"<pre><code># src/dino/loss/custom_loss.py\nimport torch\nimport torch.nn as nn\n\nclass CustomDinoLoss(nn.Module):\n    \"\"\"Custom DINO loss with additional regularization.\"\"\"\n\n    def __init__(self, base_loss, reg_weight: float = 0.1):\n        super().__init__()\n        self.base_loss = base_loss\n        self.reg_weight = reg_weight\n\n    def forward(self, student_output, teacher_output):\n        # Base DINO loss\n        loss = self.base_loss(student_output, teacher_output)\n\n        # Add regularization\n        reg = self.compute_regularization(student_output)\n        total_loss = loss + self.reg_weight * reg\n\n        return total_loss\n\n    def compute_regularization(self, output):\n        # Example: entropy regularization\n        probs = torch.softmax(output, dim=-1)\n        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=-1)\n        return -entropy.mean()  # Maximize entropy\n</code></pre>"},{"location":"advanced/extending/#see-also","title":"See Also","text":"<ul> <li>Models - Existing model architectures</li> <li>Data Pipeline - Data loading details</li> <li>API Reference - Full API documentation</li> </ul>"},{"location":"advanced/performance/","title":"Performance Optimization","text":"<p>Guide to optimizing training performance, memory usage, and speed.</p>"},{"location":"advanced/performance/#memory-optimization","title":"Memory Optimization","text":""},{"location":"advanced/performance/#multi-crop-memory-impact","title":"Multi-Crop Memory Impact","text":"<p>DINO's multi-crop strategy uses 8 views per image, significantly increasing memory usage:</p> <pre><code>Standard training: batch_size \u00d7 1 view\nDINO training:     batch_size \u00d7 8 views = 8\u00d7 memory\n</code></pre>"},{"location":"advanced/performance/#reducing-memory-usage","title":"Reducing Memory Usage","text":""},{"location":"advanced/performance/#1-reduce-batch-size","title":"1. Reduce Batch Size","text":"<pre><code>data:\n  batch_size: 16  # Instead of 64\n</code></pre>"},{"location":"advanced/performance/#2-reduce-local-crops","title":"2. Reduce Local Crops","text":"<pre><code>augmentation:\n  num_local_views: 4  # Instead of 6\n</code></pre>"},{"location":"advanced/performance/#3-use-smaller-backbone","title":"3. Use Smaller Backbone","text":"<pre><code>model:\n  backbone: resnet18  # Instead of resnet50\n</code></pre>"},{"location":"advanced/performance/#4-gradient-accumulation","title":"4. Gradient Accumulation","text":"<p>Simulate larger batch sizes without extra memory:</p> <pre><code>accumulation_steps = 4\neffective_batch_size = batch_size * accumulation_steps\n\nfor i, batch in enumerate(dataloader):\n    loss = compute_loss(batch) / accumulation_steps\n    loss.backward()\n\n    if (i + 1) % accumulation_steps == 0:\n        optimizer.step()\n        optimizer.zero_grad()\n</code></pre>"},{"location":"advanced/performance/#training-speed","title":"Training Speed","text":""},{"location":"advanced/performance/#dataloader-optimization","title":"DataLoader Optimization","text":"<pre><code>data:\n  num_workers: 8        # Match CPU cores\n  pin_memory: true      # Faster GPU transfer\n  persistent_workers: true  # Keep workers alive\n</code></pre>"},{"location":"advanced/performance/#bottleneck-analysis","title":"Bottleneck Analysis","text":"<p>Common bottlenecks and solutions:</p> Bottleneck Symptom Solution Data loading GPU utilization &lt; 90% Increase <code>num_workers</code> Augmentation CPU at 100% Cache augmented data Forward pass Normal Use mixed precision Backward pass Normal Use gradient checkpointing"},{"location":"advanced/performance/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Use FP16 to reduce memory and increase speed:</p> <pre><code>from torch.cuda.amp import autocast, GradScaler\n\nscaler = GradScaler()\n\nfor batch in dataloader:\n    optimizer.zero_grad()\n\n    # Forward pass in FP16\n    with autocast():\n        student_output = student(views)\n        teacher_output = teacher(global_views)\n        loss = loss_fn(student_output, teacher_output)\n\n    # Backward pass with scaling\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n\n    # EMA update (in FP32)\n    update_teacher_EMA(student, teacher, momentum)\n</code></pre>"},{"location":"advanced/performance/#benefits","title":"Benefits","text":"<ul> <li>Memory: ~50% reduction</li> <li>Speed: 2-3\u00d7 faster on modern GPUs</li> <li>Accuracy: Typically no degradation</li> </ul>"},{"location":"advanced/performance/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Trade compute for memory by recomputing activations:</p> <pre><code>from torch.utils.checkpoint import checkpoint\n\nclass CheckpointedBackbone(nn.Module):\n    def __init__(self, backbone):\n        super().__init__()\n        self.backbone = backbone\n\n    def forward(self, x):\n        # Recompute activations during backward\n        return checkpoint(self.backbone, x)\n</code></pre>"},{"location":"advanced/performance/#when-to-use","title":"When to Use","text":"<ul> <li>Large models (ResNet50+, ViT)</li> <li>GPU memory is the limiting factor</li> <li>Willing to trade ~20% speed for ~30% memory savings</li> </ul>"},{"location":"advanced/performance/#distributed-training","title":"Distributed Training","text":""},{"location":"advanced/performance/#data-parallel-single-node-multi-gpu","title":"Data Parallel (Single Node, Multi-GPU)","text":"<pre><code>import torch.nn as nn\n\n# Wrap student only (teacher doesn't need gradients)\nstudent = nn.DataParallel(student)\n</code></pre>"},{"location":"advanced/performance/#distributed-data-parallel-multi-node","title":"Distributed Data Parallel (Multi-Node)","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# Initialize\ndist.init_process_group(backend='nccl')\nlocal_rank = int(os.environ['LOCAL_RANK'])\n\n# Wrap model\nstudent = DDP(student, device_ids=[local_rank])\n\n# Synchronize center in loss\nif dist.is_initialized():\n    dist.all_reduce(batch_center)\n    batch_center /= dist.get_world_size()\n</code></pre>"},{"location":"advanced/performance/#launch-command","title":"Launch Command","text":"<pre><code># Single node, 4 GPUs\ntorchrun --nproc_per_node=4 scripts/train.py\n\n# Multi-node\ntorchrun --nnodes=2 --nproc_per_node=4 \\\n    --rdzv_endpoint=master:29500 \\\n    scripts/train.py\n</code></pre>"},{"location":"advanced/performance/#profiling","title":"Profiling","text":""},{"location":"advanced/performance/#pytorch-profiler","title":"PyTorch Profiler","text":"<pre><code>from torch.profiler import profile, ProfilerActivity, tensorboard_trace_handler\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3),\n    on_trace_ready=tensorboard_trace_handler('./logs/profile'),\n    record_shapes=True,\n    profile_memory=True,\n) as prof:\n    for step, batch in enumerate(dataloader):\n        if step &gt;= 5:\n            break\n        train_step(batch)\n        prof.step()\n\n# View in TensorBoard\n# tensorboard --logdir logs/profile\n</code></pre>"},{"location":"advanced/performance/#nvidia-tools","title":"NVIDIA Tools","text":"<pre><code># Monitor GPU usage\nnvidia-smi -l 1\n\n# Detailed profiling\nnsys profile python scripts/train.py\n</code></pre>"},{"location":"advanced/performance/#hardware-recommendations","title":"Hardware Recommendations","text":""},{"location":"advanced/performance/#minimum-small-experiments","title":"Minimum (Small experiments)","text":"<ul> <li>GPU: 8GB VRAM (GTX 1080, RTX 2080)</li> <li>RAM: 16GB</li> <li>Storage: SSD recommended</li> </ul>"},{"location":"advanced/performance/#recommended-full-training","title":"Recommended (Full training)","text":"<ul> <li>GPU: 16GB+ VRAM (RTX 3090, A100)</li> <li>RAM: 32GB+</li> <li>Storage: NVMe SSD</li> </ul>"},{"location":"advanced/performance/#configurations-by-gpu","title":"Configurations by GPU","text":"GPU Batch Size Backbone Local Crops 8GB 16 ResNet18 4 12GB 32 ResNet18 6 16GB 32 ResNet50 6 24GB+ 64 ResNet50/ViT 6-8"},{"location":"advanced/performance/#quick-optimization-checklist","title":"Quick Optimization Checklist","text":"<ul> <li>[ ] Set <code>num_workers</code> to CPU core count</li> <li>[ ] Enable <code>pin_memory: true</code></li> <li>[ ] Use appropriate batch size for GPU</li> <li>[ ] Consider reducing local crops (6 \u2192 4)</li> <li>[ ] Enable mixed precision for modern GPUs</li> <li>[ ] Use gradient accumulation if batch size limited</li> <li>[ ] Profile to identify bottlenecks</li> </ul>"},{"location":"advanced/performance/#see-also","title":"See Also","text":"<ul> <li>Training - Training configuration</li> <li>Configuration - Full config reference</li> <li>Troubleshooting - Common issues</li> </ul>"},{"location":"api/data/","title":"Data API Reference","text":"<p>API documentation for the data module.</p>"},{"location":"api/data/#dinotransform","title":"DINOTransform","text":""},{"location":"api/data/#dino.data.DINOTransform","title":"<code>DINOTransform(num_local_views=6, global_crop_size=224, local_crop_size=96, global_crop_scale=(0.4, 1.0), local_crop_scale=(0.05, 0.4), color_jitter_prob=0.8, color_jitter_params=(0.4, 0.4, 0.2, 0.1), horizontal_flip_prob=0.5, grayscale_prob=0.2, gaussian_blur_sigma=(0.1, 2.0), solarization_prob=0.2, solarization_threshold=128, normalize_mean=(0.485, 0.456, 0.406), normalize_std=(0.229, 0.224, 0.225))</code>","text":"<p>Multi-crop data augmentation strategy for DINO.</p> <p>Creates multiple augmented views of the same image: - 2 global views at higher resolution - N local views at lower resolution</p> <p>This encourages the model to learn global and local features.</p> <p>Parameters:</p> Name Type Description Default <code>num_local_views</code> <code>int</code> <p>Number of local crops</p> <code>6</code> <code>global_crop_size</code> <code>int</code> <p>Size of global crops</p> <code>224</code> <code>local_crop_size</code> <code>int</code> <p>Size of local crops</p> <code>96</code> <code>global_crop_scale</code> <code>Tuple[float, float]</code> <p>Tuple of (min_scale, max_scale) for global crops</p> <code>(0.4, 1.0)</code> <code>local_crop_scale</code> <code>Tuple[float, float]</code> <p>Tuple of (min_scale, max_scale) for local crops</p> <code>(0.05, 0.4)</code> <code>color_jitter_prob</code> <code>float</code> <p>Probability of applying color jitter</p> <code>0.8</code> <code>color_jitter_params</code> <code>Tuple[float, float, float, float]</code> <p>Tuple of (brightness, contrast, saturation, hue)</p> <code>(0.4, 0.4, 0.2, 0.1)</code> <code>horizontal_flip_prob</code> <code>float</code> <p>Probability of horizontal flip</p> <code>0.5</code> <code>grayscale_prob</code> <code>float</code> <p>Probability of converting to grayscale</p> <code>0.2</code> <code>gaussian_blur_sigma</code> <code>Tuple[float, float]</code> <p>Tuple of (min_sigma, max_sigma) for Gaussian blur</p> <code>(0.1, 2.0)</code> <code>solarization_prob</code> <code>float</code> <p>Probability of applying solarization (only to 2nd global view)</p> <code>0.2</code> <code>solarization_threshold</code> <code>int</code> <p>Threshold for solarization</p> <code>128</code> <code>normalize_mean</code> <code>Tuple[float, float, float]</code> <p>Mean for normalization</p> <code>(0.485, 0.456, 0.406)</code> <code>normalize_std</code> <code>Tuple[float, float, float]</code> <p>Standard deviation for normalization</p> <code>(0.229, 0.224, 0.225)</code> Example <p>transform = DINOTransform( ...     num_local_views=6, ...     global_crop_size=224, ...     local_crop_size=96 ... ) from PIL import Image img = Image.open('image.jpg') views = transform(img) len(views) 8  # 2 global + 6 local</p> Source code in <code>src/dino/data/transforms.py</code> <pre><code>def __init__(\n    self,\n    num_local_views: int = 6,\n    global_crop_size: int = 224,\n    local_crop_size: int = 96,\n    global_crop_scale: Tuple[float, float] = (0.4, 1.0),\n    local_crop_scale: Tuple[float, float] = (0.05, 0.4),\n    color_jitter_prob: float = 0.8,\n    color_jitter_params: Tuple[float, float, float, float] = (0.4, 0.4, 0.2, 0.1),\n    horizontal_flip_prob: float = 0.5,\n    grayscale_prob: float = 0.2,\n    gaussian_blur_sigma: Tuple[float, float] = (0.1, 2.0),\n    solarization_prob: float = 0.2,\n    solarization_threshold: int = 128,\n    normalize_mean: Tuple[float, float, float] = (0.485, 0.456, 0.406),\n    normalize_std: Tuple[float, float, float] = (0.229, 0.224, 0.225),\n):\n    self.num_local_views = num_local_views\n\n    # Common augmentations for all crops\n    flip_and_color_jitter = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=horizontal_flip_prob),\n        transforms.RandomApply(\n            [transforms.ColorJitter(*color_jitter_params)],\n            p=color_jitter_prob\n        ),\n        transforms.RandomGrayscale(p=grayscale_prob),\n    ])\n\n    # Normalization\n    normalize = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std),\n    ])\n\n    # Global view 1 (no solarization)\n    self.global_t1 = transforms.Compose([\n        transforms.RandomResizedCrop(global_crop_size, scale=global_crop_scale),\n        flip_and_color_jitter,\n        transforms.GaussianBlur(kernel_size=23, sigma=gaussian_blur_sigma),\n        normalize,\n    ])\n\n    # Global view 2 (with solarization)\n    self.global_t2 = transforms.Compose([\n        transforms.RandomResizedCrop(global_crop_size, scale=global_crop_scale),\n        flip_and_color_jitter,\n        transforms.RandomApply(\n            [transforms.GaussianBlur(kernel_size=23, sigma=gaussian_blur_sigma)],\n            p=0.1\n        ),\n        transforms.RandomSolarize(threshold=solarization_threshold, p=solarization_prob),\n        normalize,\n    ])\n\n    # Local views\n    self.local_transform = transforms.Compose([\n        transforms.RandomResizedCrop(local_crop_size, scale=local_crop_scale),\n        flip_and_color_jitter,\n        transforms.RandomApply(\n            [transforms.GaussianBlur(kernel_size=23, sigma=gaussian_blur_sigma)],\n            p=0.5\n        ),\n        normalize,\n    ])\n</code></pre>"},{"location":"api/data/#dino.data.DINOTransform.__call__","title":"<code>__call__(img)</code>","text":"<p>Apply transformations to create multiple views.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <p>PIL Image</p> required <p>Returns:</p> Type Description <code>List[Tensor]</code> <p>List of augmented views (2 global + num_local_views local)</p> Source code in <code>src/dino/data/transforms.py</code> <pre><code>def __call__(self, img) -&gt; List[torch.Tensor]:\n    \"\"\"\n    Apply transformations to create multiple views.\n\n    Args:\n        img: PIL Image\n\n    Returns:\n        List of augmented views (2 global + num_local_views local)\n    \"\"\"\n    views = []\n\n    # Add 2 global views\n    views.append(self.global_t1(img))\n    views.append(self.global_t2(img))\n\n    # Add local views\n    for _ in range(self.num_local_views):\n        views.append(self.local_transform(img))\n\n    return views\n</code></pre>"},{"location":"api/data/#dino.data.DINOTransform.from_config","title":"<code>from_config(augmentation_config)</code>  <code>classmethod</code>","text":"<p>Factory method to create DINOTransform from AugmentationConfig.</p> <p>Parameters:</p> Name Type Description Default <code>augmentation_config</code> <p>AugmentationConfig instance</p> required <p>Returns:</p> Type Description <code>DINOTransform</code> <p>DINOTransform instance</p> Example <p>from dino.config.config import AugmentationConfig config = AugmentationConfig() transform = DINOTransform.from_config(config)</p> Source code in <code>src/dino/data/transforms.py</code> <pre><code>@classmethod\ndef from_config(cls, augmentation_config) -&gt; 'DINOTransform':\n    \"\"\"\n    Factory method to create DINOTransform from AugmentationConfig.\n\n    Args:\n        augmentation_config: AugmentationConfig instance\n\n    Returns:\n        DINOTransform instance\n\n    Example:\n        &gt;&gt;&gt; from dino.config.config import AugmentationConfig\n        &gt;&gt;&gt; config = AugmentationConfig()\n        &gt;&gt;&gt; transform = DINOTransform.from_config(config)\n    \"\"\"\n    return cls(\n        num_local_views=augmentation_config.num_local_views,\n        global_crop_size=augmentation_config.global_crop_size,\n        local_crop_size=augmentation_config.local_crop_size,\n        global_crop_scale=(\n            augmentation_config.global_crop_scale_min,\n            augmentation_config.global_crop_scale_max\n        ),\n        local_crop_scale=(\n            augmentation_config.local_crop_scale_min,\n            augmentation_config.local_crop_scale_max\n        ),\n        color_jitter_prob=augmentation_config.color_jitter_prob,\n        color_jitter_params=(\n            augmentation_config.color_jitter_brightness,\n            augmentation_config.color_jitter_contrast,\n            augmentation_config.color_jitter_saturation,\n            augmentation_config.color_jitter_hue\n        ),\n        horizontal_flip_prob=augmentation_config.horizontal_flip_prob,\n        grayscale_prob=augmentation_config.grayscale_prob,\n        gaussian_blur_sigma=(\n            augmentation_config.gaussian_blur_sigma_min,\n            augmentation_config.gaussian_blur_sigma_max\n        ),\n        solarization_prob=augmentation_config.solarization_prob,\n        solarization_threshold=augmentation_config.solarization_threshold,\n        normalize_mean=augmentation_config.normalize_mean,\n        normalize_std=augmentation_config.normalize_std,\n    )\n</code></pre>"},{"location":"api/data/#datasets","title":"Datasets","text":""},{"location":"api/data/#get_dataset","title":"get_dataset","text":""},{"location":"api/data/#dino.data.get_dataset","title":"<code>get_dataset(dataset_name, data_path, transform=None, download=True, train=True)</code>","text":"<p>Get dataset by name.</p> <p>Supported datasets: - imagenette: ImageNette (subset of ImageNet) - imagenet100: ImageNet100 (100-class subset of ImageNet from Kaggle)</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <code>data_path</code> <code>str</code> <p>Path to data directory</p> required <code>transform</code> <code>Optional[Callable]</code> <p>Transform to apply to images</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the dataset if not present</p> <code>True</code> <code>train</code> <code>bool</code> <p>Whether to load training or test split</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Dataset instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_name is not supported</p> Example <p>from dino.data.transforms import DINOTransform transform = DINOTransform() dataset = get_dataset('imagenette', './data', transform=transform)</p> Source code in <code>src/dino/data/datasets.py</code> <pre><code>def get_dataset(\n    dataset_name: str,\n    data_path: str,\n    transform: Optional[Callable] = None,\n    download: bool = True,\n    train: bool = True\n) -&gt; Dataset:\n    \"\"\"\n    Get dataset by name.\n\n    Supported datasets:\n    - imagenette: ImageNette (subset of ImageNet)\n    - imagenet100: ImageNet100 (100-class subset of ImageNet from Kaggle)\n\n    Args:\n        dataset_name: Name of the dataset\n        data_path: Path to data directory\n        transform: Transform to apply to images\n        download: Whether to download the dataset if not present\n        train: Whether to load training or test split\n\n    Returns:\n        Dataset instance\n\n    Raises:\n        ValueError: If dataset_name is not supported\n\n    Example:\n        &gt;&gt;&gt; from dino.data.transforms import DINOTransform\n        &gt;&gt;&gt; transform = DINOTransform()\n        &gt;&gt;&gt; dataset = get_dataset('imagenette', './data', transform=transform)\n    \"\"\"\n    dataset_name = dataset_name.lower()\n\n    if dataset_name == 'imagenette':\n        # Imagenette uses 'train' or 'val' as split argument\n        split = 'train' if train else 'val'\n        imagenette = torchvision.datasets.Imagenette(\n            root=data_path,\n            split=split,\n            download=download,\n            transform=transform\n        )\n        logger.info(f\"Loaded Imagenette dataset ({split}) with {len(imagenette)} samples\")\n        return imagenette\n\n    elif dataset_name == 'imagenet100':\n        # ImageNet100 from Kaggle: https://www.kaggle.com/datasets/ambityga/imagenet100\n        # Structure: data_path/train.X1/, train.X2/, train.X3/, train.X4/ and data_path/val.X/\n        if train:\n            # Find all train.X* folders and combine them\n            train_patterns = sorted(glob.glob(os.path.join(data_path, 'train.X*')))\n            if not train_patterns:\n                raise FileNotFoundError(\n                    f\"ImageNet100 train folders not found at: {data_path}\\n\"\n                    f\"Expected folders like train.X1, train.X2, etc.\\n\"\n                    f\"Please download the dataset from Kaggle:\\n\"\n                    f\"  kaggle datasets download -d ambityga/imagenet100\\n\"\n                    f\"Then extract it to: {data_path}\"\n                )\n\n            logger.info(f\"Loading ImageNet100 train splits from: {train_patterns}\")\n            datasets: List[Dataset] = [\n                ImageFolder(root=folder, transform=transform)\n                for folder in train_patterns\n            ]\n            concated = ConcatDataset(datasets)\n            logger.info(f\"Loaded ImageNet100 train split with {len(concated)} samples\")\n            return concated\n        else:\n            # Validation split\n            val_path = os.path.join(data_path, 'val.X')\n            if not os.path.exists(val_path):\n                raise FileNotFoundError(\n                    f\"ImageNet100 val folder not found at: {val_path}\\n\"\n                    f\"Please download the dataset from Kaggle:\\n\"\n                    f\"  kaggle datasets download -d ambityga/imagenet100\\n\"\n                    f\"Then extract it to: {data_path}\"\n                )\n\n            logger.info(f\"Loading ImageNet100 val split from: {val_path}\")\n            val_dataset = ImageFolder(root=val_path, transform=transform)\n            logger.info(f\"Loaded ImageNet100 val split with {len(val_dataset)} samples\")\n            return val_dataset\n    else:\n        raise ValueError(\n            f\"Unknown dataset: {dataset_name}. \"\n            f\"Supported datasets: imagenette, imagenet100\"\n        )\n</code></pre>"},{"location":"api/data/#dataloaders","title":"DataLoaders","text":""},{"location":"api/data/#create_dataloaders","title":"create_dataloaders","text":""},{"location":"api/data/#dino.data.create_dataloaders","title":"<code>create_dataloaders(config, return_test=False)</code>","text":"<p>Create train, validation, and optionally test dataloaders from config.</p> <p>Supports both standard datasets (downloaded) and streaming datasets (HuggingFace). Set config.data.streaming=True to use streaming mode.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>DinoConfig instance</p> required <code>return_test</code> <code>bool</code> <p>Whether to return test dataloader</p> <code>False</code> <p>Returns:</p> Type Description <code>DataLoader</code> <p>Tuple of (train_loader, val_loader, test_loader)</p> <code>Optional[DataLoader]</code> <p>If return_test is False, test_loader will be None</p> Example <p>from dino.config.config import DinoConfig config = DinoConfig.from_yaml('configs/default.yaml') train_loader, val_loader, _ = create_dataloaders(config) for views, labels in train_loader: ...     print(len(views))  # 8 (2 global + 6 local) ...     break</p> Source code in <code>src/dino/data/dataloaders.py</code> <pre><code>def create_dataloaders(\n    config,\n    return_test: bool = False\n) -&gt; Tuple[DataLoader, Optional[DataLoader], Optional[DataLoader]]:\n    \"\"\"\n    Create train, validation, and optionally test dataloaders from config.\n\n    Supports both standard datasets (downloaded) and streaming datasets (HuggingFace).\n    Set config.data.streaming=True to use streaming mode.\n\n    Args:\n        config: DinoConfig instance\n        return_test: Whether to return test dataloader\n\n    Returns:\n        Tuple of (train_loader, val_loader, test_loader)\n        If return_test is False, test_loader will be None\n\n    Example:\n        &gt;&gt;&gt; from dino.config.config import DinoConfig\n        &gt;&gt;&gt; config = DinoConfig.from_yaml('configs/default.yaml')\n        &gt;&gt;&gt; train_loader, val_loader, _ = create_dataloaders(config)\n        &gt;&gt;&gt; for views, labels in train_loader:\n        ...     print(len(views))  # 8 (2 global + 6 local)\n        ...     break\n    \"\"\"\n\n    # Create transform\n    transform = DINOTransform.from_config(config.augmentation)\n\n    # Load dataset with transform\n    dataset = get_dataset(\n        dataset_name=config.data.dataset,\n        data_path=config.data.data_path,\n        transform=transform,\n        download=True,\n        train=True\n    )\n\n    logger.info(f\"Loaded {config.data.dataset} dataset with {len(dataset)} samples\")\n\n    # Split into train/val/test\n    train_dataset, val_dataset, test_dataset = create_train_val_test_splits(\n        dataset,\n        train_split=config.data.train_split,\n        val_split=config.data.val_split,\n        seed=config.data.seed\n    )\n\n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.data.batch_size,\n        shuffle=True,\n        num_workers=config.data.num_workers,\n        pin_memory=config.data.pin_memory,\n        collate_fn=collate_multi_crop,\n        drop_last=True  # Drop last incomplete batch for stability\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config.data.batch_size,\n        shuffle=False,\n        num_workers=config.data.num_workers,\n        pin_memory=config.data.pin_memory,\n        collate_fn=collate_multi_crop,\n        drop_last=False\n    ) if len(val_dataset) &gt; 0 else None\n\n    test_loader = None\n    if return_test and len(test_dataset) &gt; 0:\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=config.data.batch_size,\n            shuffle=False,\n            num_workers=config.data.num_workers,\n            pin_memory=config.data.pin_memory,\n            collate_fn=collate_multi_crop,\n            drop_last=False\n        )\n\n    logger.info(\n        f\"Created dataloaders: \"\n        f\"train={len(train_loader)} batches, \"\n        f\"val={len(val_loader) if val_loader else 0} batches, \"\n        f\"test={len(test_loader) if test_loader else 0} batches\"\n    )\n\n    return train_loader, val_loader, test_loader\n</code></pre>"},{"location":"api/data/#collate_multi_crop","title":"collate_multi_crop","text":""},{"location":"api/data/#dino.data.collate_multi_crop","title":"<code>collate_multi_crop(batch)</code>","text":"<p>Custom collate function for multi-crop batches.</p> <p>Converts list of (views_list, label) tuples into proper batch format.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>List of (views_list, label) tuples</p> required <p>Returns:</p> Type Description <p>Tuple of (views_batch, labels_batch) where: - views_batch is a list of tensors, one per view type - labels_batch is a tensor of labels</p> Source code in <code>src/dino/data/dataloaders.py</code> <pre><code>def collate_multi_crop(batch):\n    \"\"\"\n    Custom collate function for multi-crop batches.\n\n    Converts list of (views_list, label) tuples into proper batch format.\n\n    Args:\n        batch: List of (views_list, label) tuples\n\n    Returns:\n        Tuple of (views_batch, labels_batch) where:\n            - views_batch is a list of tensors, one per view type\n            - labels_batch is a tensor of labels\n    \"\"\"\n    import torch\n\n    # batch is a list of (views_list, label)\n    # views_list contains [global1, global2, local1, ..., local6]\n\n    views_lists = [item[0] for item in batch]\n    labels = torch.tensor([item[1] for item in batch])\n\n    # Transpose: from list of lists to list of batches\n    # [[g1_img1, g2_img1, l1_img1, ...], [g1_img2, g2_img2, l2_img2, ...]]\n    # -&gt; [[g1_img1, g1_img2, ...], [g2_img1, g2_img2, ...], ...]\n    num_views = len(views_lists[0])\n    views_batch = []\n\n    for view_idx in range(num_views):\n        view_batch = torch.stack([views[view_idx] for views in views_lists])\n        views_batch.append(view_batch)\n\n    return views_batch, labels\n</code></pre>"},{"location":"api/data/#usage-examples","title":"Usage Examples","text":""},{"location":"api/data/#creating-dataloaders-from-config","title":"Creating DataLoaders from Config","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.data import create_dataloaders\n\nconfig = DinoConfig.from_yaml('configs/default.yaml')\ntrain_loader, val_loader, test_loader = create_dataloaders(config)\n\nfor views, labels in train_loader:\n    # views: list of 8 tensors, each [batch, 3, H, W]\n    # labels: [batch]\n    print(f\"Number of views: {len(views)}\")\n    print(f\"Global view shape: {views[0].shape}\")\n    print(f\"Local view shape: {views[2].shape}\")\n    break\n</code></pre>"},{"location":"api/data/#using-dinotransform","title":"Using DINOTransform","text":"<pre><code>from dino.data import DINOTransform\nfrom PIL import Image\n\n# Create transform\ntransform = DINOTransform(\n    num_local_views=6,\n    global_crop_size=224,\n    local_crop_size=96\n)\n\n# Apply to image\nimage = Image.open('example.jpg')\nviews = transform(image)\n\nprint(f\"Number of views: {len(views)}\")  # 8\nprint(f\"Global view shape: {views[0].shape}\")  # [3, 224, 224]\nprint(f\"Local view shape: {views[2].shape}\")  # [3, 96, 96]\n</code></pre>"},{"location":"api/data/#creating-transform-from-config","title":"Creating Transform from Config","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.data import DINOTransform\n\nconfig = DinoConfig.from_yaml('configs/default.yaml')\ntransform = DINOTransform.from_config(config.augmentation)\n</code></pre>"},{"location":"api/data/#getting-datasets","title":"Getting Datasets","text":"<pre><code>from dino.data import get_dataset, DINOTransform\n\ntransform = DINOTransform(num_local_views=6)\n\n# ImageNette\ndataset = get_dataset(\n    name='imagenette',\n    path='./data',\n    transform=transform,\n    split='train'\n)\n\n# ImageNet100\ndataset = get_dataset(\n    name='imagenet100',\n    path='./data/imagenet100',\n    transform=transform,\n    split='train'\n)\n</code></pre>"},{"location":"api/data/#custom-collate-function","title":"Custom Collate Function","text":"<pre><code>from dino.data import collate_multi_crop\nfrom torch.utils.data import DataLoader\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    collate_fn=collate_multi_crop,\n    num_workers=4,\n    pin_memory=True\n)\n</code></pre>"},{"location":"api/loss/","title":"Loss API Reference","text":"<p>API documentation for the loss module.</p>"},{"location":"api/loss/#dinoloss","title":"DinoLoss","text":""},{"location":"api/loss/#dino.loss.DinoLoss","title":"<code>DinoLoss(out_dim, student_temp=0.1, teacher_temp=0.04, center_momentum=0.9, n_global_crops=2, ncrops=8)</code>","text":"<p>               Bases: <code>Module</code></p> <p>DINO loss with cross-entropy, temperature scaling, and centering.</p> <p>The loss encourages the student to match the teacher's output distribution on different augmented views of the same image. Key features: - Temperature scaling: Different temperatures for student (sharper) and teacher (softer) - Centering: Running mean of teacher outputs to prevent collapse - Cross-entropy: Between student log-probabilities and teacher probabilities</p> <p>Parameters:</p> Name Type Description Default <code>out_dim</code> <code>int</code> <p>Output dimension of the projection head</p> required <code>student_temp</code> <code>float</code> <p>Temperature for student (lower = sharper, typical: 0.1)</p> <code>0.1</code> <code>teacher_temp</code> <code>float</code> <p>Temperature for teacher (lower = more confident, typical: 0.04)</p> <code>0.04</code> <code>center_momentum</code> <code>float</code> <p>EMA momentum for centering (typical: 0.9)</p> <code>0.9</code> <code>n_global_crops</code> <code>int</code> <p>Number of global crops (typically 2)</p> <code>2</code> <code>ncrops</code> <code>int</code> <p>Total number of crops (global + local, typically 8)</p> <code>8</code> Example <p>loss_fn = DinoLoss(out_dim=2048, n_global_crops=2, ncrops=8) student_out = torch.randn(256, 2048)  # 8 views * 32 batch_size teacher_out = torch.randn(64, 2048)   # 2 views * 32 batch_size loss = loss_fn(student_out, teacher_out) print(loss.item() &gt; 0)  # Should be positive True</p> Source code in <code>src/dino/loss/dino_loss.py</code> <pre><code>def __init__(\n    self,\n    out_dim: int,\n    student_temp: float = 0.1,\n    teacher_temp: float = 0.04,\n    center_momentum: float = 0.9,\n    n_global_crops: int = 2,\n    ncrops: int = 8\n):\n    super().__init__()\n\n    if student_temp &lt;= 0 or teacher_temp &lt;= 0:\n        raise ValueError(\"Temperatures must be positive\")\n    if student_temp &lt;= teacher_temp:\n        logger.warning(\n            f\"Student temperature ({student_temp}) should typically be \"\n            f\"higher than teacher temperature ({teacher_temp})\"\n        )\n\n    self.student_temp = student_temp\n    self.teacher_temp = teacher_temp\n    self.n_global_crops = n_global_crops\n    self.ncrops = ncrops\n    self.center_momentum = center_momentum\n\n    # Register a buffer to store the center for teacher output normalization\n    # Buffer is not a parameter, but will be saved in state_dict\n    self.register_buffer(\"center\", torch.zeros(1, out_dim))\n</code></pre>"},{"location":"api/loss/#dino.loss.DinoLoss.forward","title":"<code>forward(student_outputs, teacher_outputs)</code>","text":"<p>Compute DINO loss.</p> <p>Parameters:</p> Name Type Description Default <code>student_outputs</code> <code>Tensor</code> <p>Student outputs of shape [ncrops * batch_size, out_dim]             Contains outputs for all views (global + local)</p> required <code>teacher_outputs</code> <code>Tensor</code> <p>Teacher outputs of shape [n_global_crops * batch_size, out_dim]             Contains outputs only for global views</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss value</p> Note <p>The loss is computed as average cross-entropy between: - Each teacher view and each student view (excluding same view pairs) This creates (n_global_crops * ncrops - n_global_crops) loss terms</p> Source code in <code>src/dino/loss/dino_loss.py</code> <pre><code>def forward(\n    self,\n    student_outputs: torch.Tensor,\n    teacher_outputs: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute DINO loss.\n\n    Args:\n        student_outputs: Student outputs of shape [ncrops * batch_size, out_dim]\n                        Contains outputs for all views (global + local)\n        teacher_outputs: Teacher outputs of shape [n_global_crops * batch_size, out_dim]\n                        Contains outputs only for global views\n\n    Returns:\n        Scalar loss value\n\n    Note:\n        The loss is computed as average cross-entropy between:\n        - Each teacher view and each student view (excluding same view pairs)\n        This creates (n_global_crops * ncrops - n_global_crops) loss terms\n    \"\"\"\n    # Apply temperature scaling and compute log-probabilities for student\n    student_logits = student_outputs / self.student_temp\n    student_log_probs = F.log_softmax(student_logits, dim=-1)\n\n    # Chunk into individual views\n    student_log_probs_chunked = student_log_probs.chunk(self.ncrops)\n\n    # Apply centering and temperature scaling for teacher\n    teacher_logits = (teacher_outputs - self.center) / self.teacher_temp\n    teacher_probs = F.softmax(teacher_logits, dim=-1)\n\n    # Detach teacher probabilities (no gradients through teacher)\n    teacher_probs = teacher_probs.detach()\n\n    # Chunk into individual views\n    teacher_probs_chunked = teacher_probs.chunk(self.n_global_crops)\n\n    # Compute cross-entropy loss between all view pairs\n    total_loss = 0.0\n    n_loss_terms = 0\n\n    # Use the chunked probability distributions, not raw outputs\n    for i, teacher_prob in enumerate(teacher_probs_chunked):\n        for j, student_log_prob in enumerate(student_log_probs_chunked):\n            # Skip when comparing same views\n            if i == j:\n                continue\n\n            # Cross-entropy: -sum(p * log_q)\n            # Shape: [batch_size, out_dim] -&gt; [batch_size] -&gt; scalar\n            loss = -torch.sum(teacher_prob * student_log_prob, dim=-1)\n            total_loss += loss.mean()\n            n_loss_terms += 1\n\n    # Average over all loss terms\n    total_loss /= n_loss_terms\n\n    # Update the center with EMA\n    self.update_center(teacher_outputs)\n\n    return total_loss\n</code></pre>"},{"location":"api/loss/#dino.loss.DinoLoss.from_config","title":"<code>from_config(loss_config, aug_config, out_dim)</code>  <code>classmethod</code>","text":"<p>Create a DinoLoss from configuration dataclasses.</p> <p>Parameters:</p> Name Type Description Default <code>loss_config</code> <code>LossConfig</code> <p>Loss configuration dataclass</p> required <code>aug_config</code> <code>AugmentationConfig</code> <p>Augmentation configuration dataclass</p> required <code>out_dim</code> <code>int</code> <p>Output dimension of the projection head</p> required <p>Returns:</p> Type Description <code>DinoLoss</code> <p>Configured DinoLoss instance</p> Source code in <code>src/dino/loss/dino_loss.py</code> <pre><code>@classmethod\ndef from_config(\n    cls,\n    loss_config: LossConfig,\n    aug_config: AugmentationConfig,\n    out_dim: int\n) -&gt; DinoLoss:\n    \"\"\"\n    Create a DinoLoss from configuration dataclasses.\n\n    Args:\n        loss_config: Loss configuration dataclass\n        aug_config: Augmentation configuration dataclass\n        out_dim: Output dimension of the projection head\n\n    Returns:\n        Configured DinoLoss instance\n    \"\"\"\n    return cls(\n        out_dim=out_dim,\n        student_temp=loss_config.student_temp,\n        teacher_temp=loss_config.teacher_temp,\n        center_momentum=loss_config.center_momentum,\n        n_global_crops=aug_config.n_global_crops,\n        ncrops=aug_config.ncrops\n    )\n</code></pre>"},{"location":"api/loss/#usage-examples","title":"Usage Examples","text":""},{"location":"api/loss/#creating-loss-from-config","title":"Creating Loss from Config","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.loss import DinoLoss\nfrom dino.models import DinoModel\n\nconfig = DinoConfig.from_yaml('configs/default.yaml')\nmodel = DinoModel.from_config(config)\n\nloss_fn = DinoLoss.from_config(\n    config.loss,\n    config.augmentation,\n    out_dim=model.output_dim\n)\n</code></pre>"},{"location":"api/loss/#manual-loss-creation","title":"Manual Loss Creation","text":"<pre><code>from dino.loss import DinoLoss\n\nloss_fn = DinoLoss(\n    out_dim=2048,\n    student_temp=0.1,\n    teacher_temp=0.04,\n    center_momentum=0.9,\n    ncrops=8,\n    n_global_crops=2\n)\n</code></pre>"},{"location":"api/loss/#computing-loss","title":"Computing Loss","text":"<pre><code>import torch\n\n# Student processes all views\nstudent_outputs = []\nfor view in all_views:  # 8 views\n    student_outputs.append(student(view))\nstudent_output = torch.cat(student_outputs, dim=0)\n\n# Teacher processes only global views\nwith torch.no_grad():\n    teacher_outputs = []\n    for view in global_views:  # 2 views\n        teacher_outputs.append(teacher(view))\n    teacher_output = torch.cat(teacher_outputs, dim=0)\n\n# Compute loss\nloss = loss_fn(student_output, teacher_output)\nloss.backward()\n</code></pre>"},{"location":"api/loss/#accessing-the-center","title":"Accessing the Center","text":"<p>The loss maintains a running center for stability:</p> <pre><code># Get current center\ncenter = loss_fn.center\n\n# The center is updated automatically during forward()\n# It's also saved/loaded with checkpoints\n</code></pre>"},{"location":"api/loss/#loss-configuration","title":"Loss Configuration","text":"<pre><code>loss:\n  student_temp: 0.1        # Higher = softer distribution\n  teacher_temp: 0.04       # Lower = sharper distribution\n  center_momentum: 0.9     # EMA momentum for centering\n</code></pre>"},{"location":"api/models/","title":"Models API Reference","text":"<p>API documentation for the models module.</p>"},{"location":"api/models/#dinomodel","title":"DinoModel","text":""},{"location":"api/models/#dino.models.DinoModel","title":"<code>DinoModel(backbone, projection_head)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Complete DINO model combining backbone and projection head.</p> <p>This model can be used for both student and teacher networks.</p> <p>Parameters:</p> Name Type Description Default <code>backbone</code> <code>Module</code> <p>Backbone network (e.g., ResNet)</p> required <code>projection_head</code> <code>Module</code> <p>Projection head network</p> required <p>Example: <pre><code>from dino.models.backbone import get_backbone\nfrom dino.models import DinoProjectionHead\nbackbone = get_backbone('resnet18')\nprojection_head = DinoProjectionHead(input_dim=backbone.output_dim)\nmodel = DinoModel(backbone, projection_head)\nx = torch.randn(2, 3, 224, 224)\noutput = model(x)\nprint(output.shape)\ntorch.Size([2, 2048])\n</code></pre></p> Source code in <code>src/dino/models/dino_model.py</code> <pre><code>def __init__(self, backbone: nn.Module, projection_head: nn.Module):\n    super().__init__()\n    self.backbone = backbone\n    self.projection_head = projection_head\n    self.output_dim = projection_head.output_dim\n</code></pre>"},{"location":"api/models/#dino.models.DinoModel.output_dim","title":"<code>output_dim = projection_head.output_dim</code>  <code>instance-attribute</code>","text":""},{"location":"api/models/#dino.models.DinoModel.forward","title":"<code>forward(x, return_backbone_features=False)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, 3, height, width]</p> required <code>return_backbone_features</code> <code>bool</code> <p>If True, return both backbone features                        and projections. Useful for evaluation.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | Tuple[Tensor, Tensor]</code> <p>If return_backbone_features is False: Projected features of shape [batch_size, output_dim]</p> <code>Tensor | Tuple[Tensor, Tensor]</code> <p>If return_backbone_features is True: Tuple of (backbone_features, projections)     backbone_features: [batch_size, backbone_dim]     projections: [batch_size, output_dim]</p> Source code in <code>src/dino/models/dino_model.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    return_backbone_features: bool = False\n) -&gt; torch.Tensor | Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Forward pass through the model.\n\n    Args:\n        x: Input tensor of shape [batch_size, 3, height, width]\n        return_backbone_features: If True, return both backbone features\n                                   and projections. Useful for evaluation.\n\n    Returns:\n        If return_backbone_features is False:\n            Projected features of shape [batch_size, output_dim]\n        If return_backbone_features is True:\n            Tuple of (backbone_features, projections)\n                backbone_features: [batch_size, backbone_dim]\n                projections: [batch_size, output_dim]\n    \"\"\"\n    # Extract features from backbone\n    features = self.backbone(x)\n\n    # Project features\n    projections = self.projection_head(features)\n\n    if return_backbone_features:\n        return features, projections\n    return projections\n</code></pre>"},{"location":"api/models/#dino.models.DinoModel.from_config","title":"<code>from_config(config)</code>  <code>classmethod</code>","text":"<p>Create a DinoModel from a DinoConfig.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DinoConfig</code> <p>Main DINO configuration dataclass</p> required <p>Returns:</p> Type Description <code>DinoModel</code> <p>Configured DinoModel instance</p> Source code in <code>src/dino/models/dino_model.py</code> <pre><code>@classmethod\ndef from_config(cls, config: DinoConfig) -&gt; DinoModel:\n    \"\"\"\n    Create a DinoModel from a DinoConfig.\n\n    Args:\n        config: Main DINO configuration dataclass\n\n    Returns:\n        Configured DinoModel instance\n    \"\"\"\n    from .backbone import get_backbone\n    from .projection_head import DinoProjectionHead\n\n    backbone = get_backbone(\n        config.model.backbone,\n        pretrained=config.model.backbone_pretrained\n    )\n    projection_head = DinoProjectionHead.from_config(\n        config.model,\n        input_dim=backbone.output_dim\n    )\n    return cls(backbone, projection_head)\n</code></pre>"},{"location":"api/models/#backbones","title":"Backbones","text":""},{"location":"api/models/#get_backbone","title":"get_backbone","text":""},{"location":"api/models/#dino.models.get_backbone","title":"<code>get_backbone(name, pretrained=False, **kwargs)</code>","text":"<p>Factory function to get backbone by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Backbone name ('resnet18', 'resnet50', 'dinov2_small', 'dinov2_base', etc.)</p> required <code>pretrained</code> <code>bool</code> <p>Whether to use pre-trained weights</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments for the backbone</p> <code>{}</code> <p>Returns:</p> Type Description <code>BackboneBase</code> <p>Backbone instance</p> Source code in <code>src/dino/models/backbone/backbone.py</code> <pre><code>def get_backbone(name: str, pretrained: bool = False, **kwargs) -&gt; BackboneBase:\n    \"\"\"\n    Factory function to get backbone by name.\n\n    Args:\n        name: Backbone name ('resnet18', 'resnet50', 'dinov2_small', 'dinov2_base', etc.)\n        pretrained: Whether to use pre-trained weights\n        **kwargs: Additional arguments for the backbone\n\n    Returns:\n        Backbone instance\n    \"\"\"\n    # Import here to avoid circular imports\n    from .resnet import ResnetBackboneDino\n    from .vit import DinoBackbone\n\n    name_lower = name.lower()\n\n    # ResNet variants\n    if name_lower in ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']:\n        return ResnetBackboneDino(variant=name_lower, pretrained=pretrained)\n\n    # DINO v1\n    elif name_lower in ['dino_vits8', 'dino_vits16', 'dino_vitb8', 'dino_vitb16']:\n        return DinoBackbone(variant=name_lower, pretrained=pretrained)\n\n    else:\n        raise ValueError(\n            f\"Unknown backbone: {name}. \"\n            f\"Available: resnet18/34/50/101/152, dino_vits8/vits16/vitb8/vitb16\"\n        )\n</code></pre>"},{"location":"api/models/#backbonebase","title":"BackboneBase","text":""},{"location":"api/models/#dino.models.backbone.BackboneBase","title":"<code>BackboneBase()</code>","text":"<p>               Bases: <code>Module</code></p> <p>Base class for backbone architectures.</p> Source code in <code>src/dino/models/backbone/backbone.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.output_dim = None\n</code></pre>"},{"location":"api/models/#dino.models.backbone.BackboneBase.get_output_dim","title":"<code>get_output_dim()</code>","text":"<p>Get the output dimension of the backbone.</p> Source code in <code>src/dino/models/backbone/backbone.py</code> <pre><code>def get_output_dim(self) -&gt; int:\n    \"\"\"Get the output dimension of the backbone.\"\"\"\n    return self.output_dim\n</code></pre>"},{"location":"api/models/#resnetbackbonedino","title":"ResnetBackboneDino","text":""},{"location":"api/models/#dino.models.backbone.ResnetBackboneDino","title":"<code>ResnetBackboneDino(variant='resnet18', pretrained=False)</code>","text":"<p>               Bases: <code>BackboneBase</code></p> <p>ResNet backbone for DINO.</p> <p>Removes the final fully connected layer and uses the penultimate layer features as output.</p> <p>Parameters:</p> Name Type Description Default <code>variant</code> <code>str</code> <p>ResNet variant ('resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152')</p> <code>'resnet18'</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pre-trained ImageNet weights</p> <code>False</code> Source code in <code>src/dino/models/backbone/resnet.py</code> <pre><code>def __init__(self, variant: str = \"resnet18\", pretrained: bool = False):\n    super().__init__()\n\n    # Get the appropriate ResNet model\n    if variant == \"resnet18\":\n        model = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n    elif variant == \"resnet34\":\n        model = models.resnet34(weights='IMAGENET1K_V1' if pretrained else None)\n    elif variant == \"resnet50\":\n        model = models.resnet50(weights='IMAGENET1K_V2' if pretrained else None)\n    elif variant == \"resnet101\":\n        model = models.resnet101(weights='IMAGENET1K_V2' if pretrained else None)\n    elif variant == \"resnet152\":\n        model = models.resnet152(weights='IMAGENET1K_V2' if pretrained else None)\n    else:\n        raise ValueError(f\"Unknown ResNet variant: {variant}\")\n\n    # Store output dimension\n    self.output_dim = model.fc.in_features\n\n    # Remove the final fully connected layer\n    model.fc = nn.Identity()\n    self.model = model\n    self.variant = variant\n</code></pre>"},{"location":"api/models/#dino.models.backbone.ResnetBackboneDino.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the backbone.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, 3, height, width]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Feature tensor of shape [batch_size, output_dim]</p> Source code in <code>src/dino/models/backbone/resnet.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the backbone.\n\n    Args:\n        x: Input tensor of shape [batch_size, 3, height, width]\n\n    Returns:\n        Feature tensor of shape [batch_size, output_dim]\n    \"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"api/models/#dinobackbone-vit","title":"DinoBackbone (ViT)","text":""},{"location":"api/models/#dino.models.backbone.DinoBackbone","title":"<code>DinoBackbone(variant='dino_vits16', pretrained=False)</code>","text":"<p>               Bases: <code>BackboneBase</code></p> <p>DINO (v1) backbone using HuggingFace transformers.</p> <p>Uses the CLS token from the last hidden state as output.</p> <p>Parameters:</p> Name Type Description Default <code>variant</code> <code>str</code> <p>DINO variant ('dino_vits8', 'dino_vits16', 'dino_vitb8', 'dino_vitb16')</p> <code>'dino_vits16'</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pre-trained weights</p> <code>False</code> Source code in <code>src/dino/models/backbone/vit.py</code> <pre><code>def __init__(self, variant: str = \"dino_vits16\", pretrained: bool = False):\n    super().__init__()\n\n    if variant not in self.VARIANT_MAP:\n        raise ValueError(\n            f\"Unknown DINO variant: {variant}. \"\n            f\"Available: {list(self.VARIANT_MAP.keys())}\"\n        )\n\n    model_name = self.VARIANT_MAP[variant]\n\n    if pretrained:\n        self.model = ViTModel.from_pretrained(model_name)\n    else:\n        config = ViTConfig.from_pretrained(model_name)\n        self.model = ViTModel(config)\n\n    self.output_dim = self.model.config.hidden_size\n    self.variant = variant\n</code></pre>"},{"location":"api/models/#dino.models.backbone.DinoBackbone.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the backbone.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape [batch_size, 3, height, width]</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Feature tensor of shape [batch_size, output_dim]</p> Source code in <code>src/dino/models/backbone/vit.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the backbone.\n\n    Args:\n        x: Input tensor of shape [batch_size, 3, height, width]\n\n    Returns:\n        Feature tensor of shape [batch_size, output_dim]\n    \"\"\"\n    outputs = self.model(x, interpolate_pos_encoding=True)\n    cls_token = outputs.last_hidden_state[:, 0]\n    return cls_token\n</code></pre>"},{"location":"api/models/#projection-head","title":"Projection Head","text":""},{"location":"api/models/#dinoprojectionhead","title":"DinoProjectionHead","text":""},{"location":"api/models/#dino.models.DinoProjectionHead","title":"<code>DinoProjectionHead(input_dim=512, output_dim=2048, hidden_dim=1024, bottleneck_dim=256, use_weight_norm=True)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Projection head for DINO as described in the paper.</p> <p>This is a backbone-agnostic MLP that works with any feature extractor (ResNet, ViT, etc.).</p> Architecture <ul> <li>3-layer MLP with GELU activations</li> <li>L2 normalization after bottleneck</li> <li>Weight-normalized final layer (optional)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Input dimension (backbone output dimension)</p> <code>512</code> <code>output_dim</code> <code>int</code> <p>Output dimension (default: 2048 as in paper)</p> <code>2048</code> <code>hidden_dim</code> <code>int</code> <p>Hidden layer dimension (default: 1024)</p> <code>1024</code> <code>bottleneck_dim</code> <code>int</code> <p>Bottleneck dimension before final layer (default: 256)</p> <code>256</code> <code>use_weight_norm</code> <code>bool</code> <p>Whether to use weight normalization on final layer</p> <code>True</code> Source code in <code>src/dino/models/projection_head.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int = 512,\n    output_dim: int = 2048,\n    hidden_dim: int = 1024,\n    bottleneck_dim: int = 256,\n    use_weight_norm: bool = True\n):\n    super().__init__()\n\n    if input_dim &lt;= 0 or output_dim &lt;= 0 or hidden_dim &lt;= 0 or bottleneck_dim &lt;= 0:\n        raise ValueError(\"All dimensions must be positive\")\n\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.bottleneck_dim = bottleneck_dim\n\n    # 3-layer MLP\n    self.mlp = nn.Sequential(\n        nn.Linear(input_dim, hidden_dim),\n        nn.GELU(),\n        nn.Linear(hidden_dim, hidden_dim),\n        nn.GELU(),\n        nn.Linear(hidden_dim, bottleneck_dim)\n    )\n\n    # Final layer (with optional weight normalization)\n    last_layer = nn.Linear(bottleneck_dim, output_dim, bias=False)\n\n    if use_weight_norm:\n        self.last_layer = torch.nn.utils.parametrizations.weight_norm(last_layer)\n    else:\n        self.last_layer = last_layer\n</code></pre>"},{"location":"api/models/#dino.models.DinoProjectionHead.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through projection head.</p> Source code in <code>src/dino/models/projection_head.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through projection head.\"\"\"\n    # Pass through MLP\n    x = self.mlp(x)\n\n    # L2 normalize after bottleneck (as per DINO paper)\n    x = F.normalize(x, dim=-1, p=2)\n\n    # Final projection (weight-normalized layer)\n    x = self.last_layer(x)\n\n    return x\n</code></pre>"},{"location":"api/models/#dino.models.DinoProjectionHead.from_config","title":"<code>from_config(model_config, input_dim)</code>  <code>classmethod</code>","text":"<p>Factory function to create a DinoProjectionHead from a ModelConfig.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>ModelConfig</code> <p>Model configuration dataclass</p> required <code>input_dim</code> <code>int</code> <p>Input dimension (backbone output dimension)</p> required <p>Returns:</p> Type Description <code>DinoProjectionHead</code> <p>Configured DinoProjectionHead instance</p> Source code in <code>src/dino/models/projection_head.py</code> <pre><code>@classmethod\ndef from_config(cls, model_config: ModelConfig, input_dim: int) -&gt; DinoProjectionHead:\n    \"\"\"\n    Factory function to create a DinoProjectionHead from a ModelConfig.\n\n    Args:\n        model_config: Model configuration dataclass\n        input_dim: Input dimension (backbone output dimension)\n\n    Returns:\n        Configured DinoProjectionHead instance\n    \"\"\"\n    return cls(\n        input_dim=input_dim,\n        output_dim=model_config.projection_output_dim,\n        hidden_dim=model_config.projection_hidden_dim,\n        bottleneck_dim=model_config.projection_bottleneck_dim,\n        use_weight_norm=model_config.use_weight_norm\n    )\n</code></pre>"},{"location":"api/models/#usage-examples","title":"Usage Examples","text":""},{"location":"api/models/#creating-a-model-from-config","title":"Creating a Model from Config","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.models import DinoModel\n\nconfig = DinoConfig.from_yaml('configs/default.yaml')\nmodel = DinoModel.from_config(config)\n\nprint(f\"Output dimension: {model.output_dim}\")\n</code></pre>"},{"location":"api/models/#manual-model-creation","title":"Manual Model Creation","text":"<pre><code>from dino.models import get_backbone, DinoProjectionHead, DinoModel\n\n# Create components\nbackbone = get_backbone('resnet18', pretrained=True)\nprojection = DinoProjectionHead(\n    input_dim=backbone.output_dim,\n    hidden_dim=1024,\n    bottleneck_dim=256,\n    output_dim=2048\n)\nmodel = DinoModel(backbone, projection)\n</code></pre>"},{"location":"api/models/#using-different-backbones","title":"Using Different Backbones","text":"<pre><code>from dino.models import get_backbone\n\n# ResNet variants\nresnet18 = get_backbone('resnet18')\nresnet50 = get_backbone('resnet50', pretrained=True)\n\n# ViT variants\nvit_small = get_backbone('dino_vits16', pretrained=True)\nvit_base = get_backbone('dino_vitb16', pretrained=True)\n</code></pre>"},{"location":"api/models/#forward-pass","title":"Forward Pass","text":"<pre><code># Standard forward\nprojections = model(images)  # [batch, output_dim]\n\n# Get backbone features for evaluation\nfeatures, projections = model(images, return_backbone_features=True)\n# features: [batch, backbone_dim]\n# projections: [batch, output_dim]\n</code></pre>"},{"location":"api/training/","title":"Training API Reference","text":"<p>API documentation for the training module.</p>"},{"location":"api/training/#dinotrainer","title":"DinoTrainer","text":""},{"location":"api/training/#dino.training.DinoTrainer","title":"<code>DinoTrainer(config, student, teacher, optimizer, scheduler, loss_fn, train_loader, val_loader=None, device='cuda')</code>","text":"<p>DINO Trainer for self-supervised learning.</p> <p>Handles the complete training loop including: - Forward passes through student and teacher - Loss computation - EMA updates for teacher - Checkpointing - Logging</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <p>Training configuration</p> required <code>student</code> <code>Module</code> <p>Student model</p> required <code>teacher</code> <code>Module</code> <p>Teacher model</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer</p> required <code>loss_fn</code> <code>Module</code> <p>Loss function</p> required <code>train_loader</code> <code>DataLoader</code> <p>Training data loader</p> required <code>val_loader</code> <code>Optional[DataLoader]</code> <p>Optional validation data loader</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to train on</p> <code>'cuda'</code> Example <p>trainer = DinoTrainer(config, student, teacher, optimizer, loss_fn, train_loader) trainer.train(num_epochs=100)</p> Source code in <code>src/dino/training/trainer.py</code> <pre><code>def __init__(\n    self,\n    config,\n    student: nn.Module,\n    teacher: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    scheduler: Optional[torch.optim.lr_scheduler.LRScheduler],\n    loss_fn: nn.Module,\n    train_loader: DataLoader,\n    val_loader: Optional[DataLoader] = None,\n    device: str = 'cuda'\n):\n    self.config = config\n    self.student = student\n    self.teacher = teacher\n    self.optimizer = optimizer\n    self.scheduler = scheduler\n    self.loss_fn = loss_fn\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.device = device\n\n    # Check if we're in streaming mode (IterableDataset doesn't have __len__)\n    self.is_streaming = not hasattr(train_loader.dataset, '__len__')\n\n    # Training state\n    self.current_epoch = 0\n    self.current_iteration = 0\n    self.history = History(metadata={\n        'config': config.to_dict() if hasattr(config, 'to_dict') else str(config),\n        'device': device,\n    })\n\n    # Determine number of iterations per epoch\n    if self.is_streaming:\n        # For streaming, use config value or estimate based on ImageNet100 size\n        streaming_samples = getattr(config.data, 'streaming_train_samples', None)\n        if streaming_samples:\n            self.niter_per_epoch = streaming_samples // config.data.batch_size\n        else:\n            # Default estimate for ImageNet100: ~117k train samples\n            self.niter_per_epoch = 117000 // config.data.batch_size\n        logger.info(f\"Streaming mode: estimated {self.niter_per_epoch} iterations per epoch\")\n    else:\n        self.niter_per_epoch = len(train_loader)\n\n    # EMA momentum schedule\n    if config.training.teacher_momentum_schedule:\n        self.momentum_schedule = get_momentum_schedule(\n            base_momentum=config.training.teacher_momentum,\n            final_momentum=config.training.teacher_momentum_final,\n            num_epochs=config.training.num_epochs,\n            niter_per_epoch=self.niter_per_epoch\n        )\n    else:\n        self.momentum_schedule = None\n\n    # Move models to device\n    self.student.to(device)\n    self.teacher.to(device)\n    self.loss_fn.to(device)\n\n    logger.info(f\"Trainer initialized on device: {device}\")\n</code></pre>"},{"location":"api/training/#dino.training.DinoTrainer.train","title":"<code>train(num_epochs=None)</code>","text":"<p>Train for multiple epochs.</p> <p>Parameters:</p> Name Type Description Default <code>num_epochs</code> <code>Optional[int]</code> <p>Number of epochs to train (uses config if not specified)</p> <code>None</code> Source code in <code>src/dino/training/trainer.py</code> <pre><code>def train(self, num_epochs: Optional[int] = None):\n    \"\"\"\n    Train for multiple epochs.\n\n    Args:\n        num_epochs: Number of epochs to train (uses config if not specified)\n    \"\"\"\n    if num_epochs is None:\n        num_epochs = self.config.training.num_epochs\n\n    logger.info(f\"Starting training for {num_epochs} epochs\")\n    logger.info(f\"Checkpoint directory: {self.config.checkpoint.checkpoint_dir}\")\n\n    for epoch in range(self.current_epoch + 1, self.current_epoch + num_epochs + 1):\n        # Train for one epoch\n        metrics = self.train_epoch(epoch)\n\n        # Record epoch metrics\n        self.history.record_epoch(epoch, metrics)\n\n        # Log metrics\n        log_metrics(metrics, epoch, prefix=\"Train\")\n\n        # Save checkpoint\n        if epoch % self.config.checkpoint.save_every_n_epochs == 0:\n            save_checkpoint(\n                student=self.student,\n                teacher=self.teacher,\n                optimizer=self.optimizer,\n                dino_loss=self.loss_fn,\n                epoch=epoch,\n                iteration=self.current_iteration,\n                config=self.config,\n                metrics=metrics,\n                checkpoint_dir=self.config.checkpoint.checkpoint_dir,\n                history=self.history\n            )\n\n        self.current_epoch = epoch\n\n    logger.info(\"Training completed!\")\n    logger.info(f\"Total iterations: {self.current_iteration}\")\n</code></pre>"},{"location":"api/training/#dino.training.DinoTrainer.train_epoch","title":"<code>train_epoch(epoch)</code>","text":"<p>Train for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Current epoch number</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of metrics for this epoch</p> Source code in <code>src/dino/training/trainer.py</code> <pre><code>def train_epoch(self, epoch: int) -&gt; Dict[str, float]:\n    \"\"\"\n    Train for one epoch.\n\n    Args:\n        epoch: Current epoch number\n\n    Returns:\n        Dictionary of metrics for this epoch\n    \"\"\"\n    self.student.train()\n    self.teacher.eval()\n\n    epoch_loss = 0.0\n    num_batches = self.niter_per_epoch if self.is_streaming else len(self.train_loader)\n\n    accumulation_steps = self.config.training.gradient_accumulation_steps\n\n    # Progress bar (with total if known, otherwise streaming mode)\n    pbar = tqdm(\n        self.train_loader,\n        desc=f\"Epoch {epoch}/{self.config.training.num_epochs}\",\n        leave=True,\n        total=num_batches if not self.is_streaming else None\n    )\n\n    self.optimizer.zero_grad()\n\n    for batch_idx, (view_set, _) in enumerate(pbar):\n        # Move views to device\n        view_set = [v.to(self.device) for v in view_set]\n\n        # Get global and all views\n        global_views, all_views = get_global_local_views(view_set)\n\n        # Forward pass - teacher only sees global views\n        with torch.no_grad():\n            teacher_outputs = [self.teacher(v) for v in global_views]\n            teacher_output = torch.cat(teacher_outputs, dim=0)\n\n        # Forward pass - student sees all views\n        student_outputs = [self.student(v) for v in all_views]\n        student_output = torch.cat(student_outputs, dim=0)\n\n        # Compute loss\n        loss = self.loss_fn(student_output, teacher_output)\n\n        loss = loss / accumulation_steps\n\n        # Backward pass\n        loss.backward()\n\n        epoch_loss += loss.item()\n\n        if (batch_idx + 1) % accumulation_steps == 0:\n            # Gradient clipping\n            if self.config.training.gradient_clip is not None:\n                torch.nn.utils.clip_grad_norm_(\n                    self.student.parameters(),\n                    self.config.training.gradient_clip\n                )\n\n            # Optimizer step - train student\n            self.optimizer.step()\n\n            # Reset gradients apr\u00e8s la mise \u00e0 jour\n            self.optimizer.zero_grad()\n\n            # Learning rate scheduler step\n            if self.scheduler is not None:\n                self.scheduler.step()\n\n            # EMA update for teacher\n            if self.momentum_schedule is not None:\n                momentum = self.momentum_schedule[self.current_iteration]\n            else:\n                momentum = self.config.training.teacher_momentum\n\n            update_teacher_EMA(self.student, self.teacher, alpha=momentum)\n\n            # Record iteration metrics (seulement lors des vraies updates)\n            current_lr = self.optimizer.param_groups[0]['lr']\n            self.history.record_iteration(\n                iteration=self.current_iteration,\n                metrics={\n                    'loss': loss.item(),\n                    'learning_rate': current_lr,\n                    'momentum': momentum\n                }\n            )\n            self.current_iteration += 1\n\n        # Update progress bar\n        if self.momentum_schedule is not None:\n            momentum = self.momentum_schedule[min(self.current_iteration, len(self.momentum_schedule) - 1)]\n        else:\n            momentum = self.config.training.teacher_momentum\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'momentum': f\"{momentum:.4f}\"})\n\n\n         # Log periodically\n        if (batch_idx + 1) % self.config.logging.log_every_n_iters == 0:\n            progress_str = f\"{batch_idx+1}\" if self.is_streaming else f\"{batch_idx+1}/{num_batches}\"\n            logger.info(\n                f\"Epoch {epoch} [{progress_str}] \"\n                f\"Loss: {loss.item():.4f}, \"\n                f\"Momentum: {momentum:.4f}\"\n            )\n            logger.debug(f\"Loss infos: {self.loss_fn}\")\n\n            c = self.loss_fn.get_center()\n\n            # 1. Format the first 5 elements for a quick preview\n            preview = \", \".join([f\"{x:.4f}\" for x in c.flatten()[:10].tolist()])\n\n            # 2. Log everything in one structured message\n            logger.debug(\n                f\"Center {list(c.shape)} | \"\n                f\"Norm: {c.norm():.4f} | \"\n                f\"\u03bc: {c.mean():.4f} \u00b1 {c.std():.4f} | \"\n                f\"Range: [{c.min():.4f}, {c.max():.4f}] | \"\n                f\"Data: [{preview}, ...]\"\n            )\n\n            logger.debug(f\"Gradient norms:\")\n            for name, param in self.student.named_parameters():\n                if param.requires_grad and param.grad is not None:\n                    grad_norm = param.grad.data.norm(2).item()\n                    logger.debug(f\"  {name}: {grad_norm:.4f}\")\n\n    if (batch_idx + 1) % accumulation_steps != 0:\n        if self.config.training.gradient_clip is not None:\n            torch.nn.utils.clip_grad_norm_(\n                self.student.parameters(),\n                self.config.training.gradient_clip\n            )\n\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n\n        if self.momentum_schedule is not None:\n            momentum = self.momentum_schedule[self.current_iteration]\n        else:\n            momentum = self.config.training.teacher_momentum\n\n        update_teacher_EMA(self.student, self.teacher, alpha=momentum)\n        self.current_iteration += 1\n\n    # Compute epoch metrics (use actual batch count for streaming)\n    actual_batches = batch_idx + 1\n    avg_loss = epoch_loss / actual_batches\n    metrics = {\n        'loss': avg_loss,\n        'learning_rate': self.optimizer.param_groups[0]['lr'],\n        'momentum': momentum if self.momentum_schedule else self.config.training.teacher_momentum\n    }\n\n    return metrics\n</code></pre>"},{"location":"api/training/#optimizer-helpers","title":"Optimizer Helpers","text":""},{"location":"api/training/#create_optimizer","title":"create_optimizer","text":""},{"location":"api/training/#dino.training.create_optimizer","title":"<code>create_optimizer(params, optimizer_config)</code>","text":"<p>Create an optimizer from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Iterator[Parameter]</code> <p>Model parameters to optimize</p> required <code>optimizer_config</code> <code>OptimizerConfig</code> <p>Optimizer configuration dataclass</p> required <p>Returns:</p> Type Description <code>Optimizer</code> <p>Configured optimizer instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If optimizer type is not supported</p> Source code in <code>src/dino/training/optim.py</code> <pre><code>def create_optimizer(\n    params: Iterator[torch.nn.Parameter],\n    optimizer_config: OptimizerConfig\n) -&gt; Optimizer:\n    \"\"\"\n    Create an optimizer from configuration.\n\n    Args:\n        params: Model parameters to optimize\n        optimizer_config: Optimizer configuration dataclass\n\n    Returns:\n        Configured optimizer instance\n\n    Raises:\n        ValueError: If optimizer type is not supported\n    \"\"\"\n    optimizer_name = optimizer_config.optimizer.lower()\n\n    if optimizer_name == \"adamw\":\n        return torch.optim.AdamW(\n            params,\n            lr=optimizer_config.lr,\n            weight_decay=optimizer_config.weight_decay,\n            betas=optimizer_config.betas,\n            eps=optimizer_config.eps\n        )\n    elif optimizer_name == \"adam\":\n        return torch.optim.Adam(\n            params,\n            lr=optimizer_config.lr,\n            weight_decay=optimizer_config.weight_decay,\n            betas=optimizer_config.betas,\n            eps=optimizer_config.eps\n        )\n    elif optimizer_name == \"sgd\":\n        return torch.optim.SGD(\n            params,\n            lr=optimizer_config.lr,\n            weight_decay=optimizer_config.weight_decay,\n            momentum=optimizer_config.betas[0]  # Use first beta as momentum\n        )\n    else:\n        raise ValueError(\n            f\"Unknown optimizer: {optimizer_config.optimizer}. \"\n            f\"Available: adamw, adam, sgd\"\n        )\n</code></pre>"},{"location":"api/training/#create_scheduler","title":"create_scheduler","text":""},{"location":"api/training/#dino.training.create_scheduler","title":"<code>create_scheduler(optimizer, scheduler_config, optimizer_config, total_steps, warmup_steps)</code>","text":"<p>Create a learning rate scheduler from configuration.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to schedule</p> required <code>scheduler_config</code> <code>SchedulerConfig</code> <p>Scheduler configuration dataclass</p> required <code>optimizer_config</code> <code>OptimizerConfig</code> <p>Optimizer configuration (for warmup start factor)</p> required <code>total_steps</code> <code>int</code> <p>Total number of training steps</p> required <code>warmup_steps</code> <code>int</code> <p>Number of warmup steps</p> required <p>Returns:</p> Type Description <code>LRScheduler</code> <p>Configured scheduler instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If scheduler type is not supported</p> Source code in <code>src/dino/training/optim.py</code> <pre><code>def create_scheduler(\n    optimizer: Optimizer,\n    scheduler_config: SchedulerConfig,\n    optimizer_config: OptimizerConfig,\n    total_steps: int,\n    warmup_steps: int\n) -&gt; LRScheduler:\n    \"\"\"\n    Create a learning rate scheduler from configuration.\n\n    Args:\n        optimizer: The optimizer to schedule\n        scheduler_config: Scheduler configuration dataclass\n        optimizer_config: Optimizer configuration (for warmup start factor)\n        total_steps: Total number of training steps\n        warmup_steps: Number of warmup steps\n\n    Returns:\n        Configured scheduler instance\n\n    Raises:\n        ValueError: If scheduler type is not supported\n    \"\"\"\n    scheduler_name = scheduler_config.scheduler.lower()\n\n    if scheduler_name == \"cosine_warmup\":\n        # Calculate warmup start factor\n        start_factor = scheduler_config.warmup_start_lr / optimizer_config.lr \\\n            if scheduler_config.warmup_start_lr &gt; 0 else 1e-8 / optimizer_config.lr\n\n        warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n            optimizer,\n            start_factor=start_factor,\n            end_factor=1.0,\n            total_iters=warmup_steps\n        )\n        cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=total_steps - warmup_steps,\n            eta_min=scheduler_config.min_lr\n        )\n        return torch.optim.lr_scheduler.SequentialLR(\n            optimizer,\n            schedulers=[warmup_scheduler, cosine_scheduler],\n            milestones=[warmup_steps]\n        )\n    elif scheduler_name == \"cosine\":\n        return torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=total_steps,\n            eta_min=scheduler_config.min_lr\n        )\n    elif scheduler_name == \"linear\":\n        return torch.optim.lr_scheduler.LinearLR(\n            optimizer,\n            start_factor=1.0,\n            end_factor=scheduler_config.min_lr / optimizer_config.lr,\n            total_iters=total_steps\n        )\n    elif scheduler_name == \"constant\":\n        return torch.optim.lr_scheduler.ConstantLR(\n            optimizer,\n            factor=1.0,\n            total_iters=total_steps\n        )\n    else:\n        raise ValueError(\n            f\"Unknown scheduler: {scheduler_config.scheduler}. \"\n            f\"Available: cosine_warmup, cosine, linear, constant\"\n        )\n</code></pre>"},{"location":"api/training/#usage-examples","title":"Usage Examples","text":""},{"location":"api/training/#complete-training-setup","title":"Complete Training Setup","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.models import DinoModel\nfrom dino.loss import DinoLoss\nfrom dino.training import DinoTrainer, create_optimizer, create_scheduler\nfrom dino.data import create_dataloaders\n\n# Load configuration\nconfig = DinoConfig.from_yaml('configs/default.yaml')\n\n# Create data loaders\ntrain_loader, val_loader, _ = create_dataloaders(config)\n\n# Create models\nstudent = DinoModel.from_config(config)\nteacher = DinoModel.from_config(config)\nteacher.load_state_dict(student.state_dict())\n\n# Disable gradients for teacher\nfor param in teacher.parameters():\n    param.requires_grad = False\n\n# Create loss\nloss_fn = DinoLoss.from_config(\n    config.loss,\n    config.augmentation,\n    out_dim=student.output_dim\n)\n\n# Create optimizer and scheduler\noptimizer = create_optimizer(student.parameters(), config.optimizer)\n\ntotal_steps = config.training.num_epochs * len(train_loader)\nwarmup_steps = config.scheduler.warmup_epochs * len(train_loader)\nscheduler = create_scheduler(\n    optimizer,\n    config.scheduler,\n    config.optimizer,\n    total_steps,\n    warmup_steps\n)\n\n# Create trainer\ntrainer = DinoTrainer(\n    config=config,\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loss_fn=loss_fn,\n    train_loader=train_loader\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"api/training/#creating-optimizer","title":"Creating Optimizer","text":"<pre><code>from dino.training import create_optimizer\n\noptimizer = create_optimizer(\n    model.parameters(),\n    config.optimizer\n)\n\n# config.optimizer contains:\n# - optimizer: 'adamw'\n# - lr: 0.001\n# - weight_decay: 0.04\n# - betas: [0.9, 0.999]\n</code></pre>"},{"location":"api/training/#creating-scheduler","title":"Creating Scheduler","text":"<pre><code>from dino.training import create_scheduler\n\nscheduler = create_scheduler(\n    optimizer,\n    config.scheduler,\n    config.optimizer,\n    total_steps=num_epochs * steps_per_epoch,\n    warmup_steps=warmup_epochs * steps_per_epoch\n)\n\n# config.scheduler contains:\n# - scheduler: 'cosine_warmup'\n# - warmup_epochs: 10\n# - min_lr: 1e-6\n# - warmup_start_lr: 0.0\n</code></pre>"},{"location":"api/training/#training-configuration","title":"Training Configuration","text":"<pre><code>training:\n  num_epochs: 100\n  teacher_momentum: 0.996\n  teacher_momentum_final: 1.0\n  use_momentum_schedule: true\n  gradient_clip: 3.0\n\noptimizer:\n  optimizer: adamw\n  lr: 0.001\n  weight_decay: 0.04\n  betas: [0.9, 0.999]\n\nscheduler:\n  scheduler: cosine_warmup\n  warmup_epochs: 10\n  min_lr: 1.0e-6\n  warmup_start_lr: 0.0\n</code></pre>"},{"location":"api/utils/","title":"Utils API Reference","text":"<p>API documentation for utility functions.</p>"},{"location":"api/utils/#ema","title":"EMA","text":""},{"location":"api/utils/#update_teacher_ema","title":"update_teacher_EMA","text":""},{"location":"api/utils/#dino.utils.update_teacher_EMA","title":"<code>update_teacher_EMA(student, teacher, alpha=0.99)</code>","text":"<p>Update teacher network with Exponential Moving Average of student weights.</p> <p>teacher = alpha * teacher + (1 - alpha) * student</p> <p>Parameters:</p> Name Type Description Default <code>student</code> <code>Module</code> <p>Student model</p> required <code>teacher</code> <code>Module</code> <p>Teacher model (will be updated in-place)</p> required <code>alpha</code> <code>float</code> <p>EMA momentum coefficient (typical: 0.996-0.999)</p> <code>0.99</code> Example <p>student = DinoModel(backbone, projection_head) teacher = DinoModel(backbone_copy, projection_head_copy) update_teacher_EMA(student, teacher, alpha=0.99)</p> Source code in <code>src/dino/utils/ema.py</code> <pre><code>@torch.no_grad()\ndef update_teacher_EMA(\n    student: nn.Module,\n    teacher: nn.Module,\n    alpha: float = 0.99\n):\n    \"\"\"\n    Update teacher network with Exponential Moving Average of student weights.\n\n    teacher = alpha * teacher + (1 - alpha) * student\n\n    Args:\n        student: Student model\n        teacher: Teacher model (will be updated in-place)\n        alpha: EMA momentum coefficient (typical: 0.996-0.999)\n\n    Example:\n        &gt;&gt;&gt; student = DinoModel(backbone, projection_head)\n        &gt;&gt;&gt; teacher = DinoModel(backbone_copy, projection_head_copy)\n        &gt;&gt;&gt; update_teacher_EMA(student, teacher, alpha=0.99)\n    \"\"\"\n    if not 0 &lt;= alpha &lt;= 1:\n        raise ValueError(f\"Alpha must be in [0, 1], got {alpha}\")\n\n    for student_param, teacher_param in zip(student.parameters(), teacher.parameters()):\n        teacher_param.data.mul_(alpha).add_(student_param.data, alpha=1 - alpha)\n</code></pre>"},{"location":"api/utils/#get_momentum_schedule","title":"get_momentum_schedule","text":""},{"location":"api/utils/#dino.utils.get_momentum_schedule","title":"<code>get_momentum_schedule(base_momentum, final_momentum, num_epochs, niter_per_epoch)</code>","text":"<p>Get momentum schedule that increases from base to final momentum.</p> <p>Used in DINO to gradually increase teacher momentum during training (e.g., from 0.996 to 1.0).</p> <p>Parameters:</p> Name Type Description Default <code>base_momentum</code> <code>float</code> <p>Starting momentum</p> required <code>final_momentum</code> <code>float</code> <p>Final momentum</p> required <code>num_epochs</code> <code>int</code> <p>Total number of epochs</p> required <code>niter_per_epoch</code> <code>int</code> <p>Number of iterations per epoch</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of momentum values for each iteration</p> Example <p>schedule = get_momentum_schedule(0.996, 1.0, num_epochs=100, niter_per_epoch=200) print(len(schedule)) 20000 print(schedule[0], schedule[-1]) 0.996 1.0</p> Source code in <code>src/dino/utils/ema.py</code> <pre><code>def get_momentum_schedule(\n    base_momentum: float,\n    final_momentum: float,\n    num_epochs: int,\n    niter_per_epoch: int\n) -&gt; list:\n    \"\"\"\n    Get momentum schedule that increases from base to final momentum.\n\n    Used in DINO to gradually increase teacher momentum during training\n    (e.g., from 0.996 to 1.0).\n\n    Args:\n        base_momentum: Starting momentum\n        final_momentum: Final momentum\n        num_epochs: Total number of epochs\n        niter_per_epoch: Number of iterations per epoch\n\n    Returns:\n        List of momentum values for each iteration\n\n    Example:\n        &gt;&gt;&gt; schedule = get_momentum_schedule(0.996, 1.0, num_epochs=100, niter_per_epoch=200)\n        &gt;&gt;&gt; print(len(schedule))\n        20000\n        &gt;&gt;&gt; print(schedule[0], schedule[-1])\n        0.996 1.0\n    \"\"\"\n    momentum_schedule = []\n    for epoch in range(num_epochs):\n        for _ in range(niter_per_epoch):\n            # Cosine schedule from base_momentum to final_momentum\n            progress = epoch * niter_per_epoch + len(momentum_schedule)\n            total_iters = num_epochs * niter_per_epoch\n            momentum = final_momentum - (final_momentum - base_momentum) * (\n                math.cos(math.pi * progress / total_iters) + 1\n            ) / 2\n            momentum_schedule.append(momentum)\n\n    return momentum_schedule\n</code></pre>"},{"location":"api/utils/#checkpointing","title":"Checkpointing","text":""},{"location":"api/utils/#save_checkpoint","title":"save_checkpoint","text":""},{"location":"api/utils/#dino.utils.save_checkpoint","title":"<code>save_checkpoint(student, teacher, optimizer, dino_loss, epoch, iteration, config, metrics=None, checkpoint_dir='./checkpoints', filename=None, is_best=False, history=None)</code>","text":"<p>Save a complete checkpoint of the DINO training state.</p> <p>Parameters:</p> Name Type Description Default <code>student</code> <code>Module</code> <p>Student model</p> required <code>teacher</code> <code>Module</code> <p>Teacher model</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer</p> required <code>dino_loss</code> <code>Module</code> <p>Loss module (contains center)</p> required <code>epoch</code> <code>int</code> <p>Current epoch</p> required <code>iteration</code> <code>int</code> <p>Current iteration</p> required <code>config</code> <code>Any</code> <p>Training configuration</p> required <code>metrics</code> <code>Optional[Dict[str, float]]</code> <p>Optional metrics dict</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>Directory to save checkpoints</p> <code>'./checkpoints'</code> <code>filename</code> <code>Optional[str]</code> <p>Optional custom filename</p> <code>None</code> <code>is_best</code> <code>bool</code> <p>Whether this is the best checkpoint</p> <code>False</code> <code>history</code> <code>Optional[History]</code> <p>Optional History instance to save alongside checkpoint</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to saved checkpoint</p> Source code in <code>src/dino/utils/checkpoint.py</code> <pre><code>def save_checkpoint(\n    student: torch.nn.Module,\n    teacher: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    dino_loss: torch.nn.Module,\n    epoch: int,\n    iteration: int,\n    config: Any,\n    metrics: Optional[Dict[str, float]] = None,\n    checkpoint_dir: str = './checkpoints',\n    filename: Optional[str] = None,\n    is_best: bool = False,\n    history: Optional['History'] = None\n) -&gt; Path:\n    \"\"\"\n    Save a complete checkpoint of the DINO training state.\n\n    Args:\n        student: Student model\n        teacher: Teacher model\n        optimizer: Optimizer\n        dino_loss: Loss module (contains center)\n        epoch: Current epoch\n        iteration: Current iteration\n        config: Training configuration\n        metrics: Optional metrics dict\n        checkpoint_dir: Directory to save checkpoints\n        filename: Optional custom filename\n        is_best: Whether this is the best checkpoint\n        history: Optional History instance to save alongside checkpoint\n\n    Returns:\n        Path to saved checkpoint\n    \"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n\n    # Generate filename if not provided\n    if filename is None:\n        filename = f'checkpoint_epoch_{epoch:04d}.pth'\n\n    checkpoint_path = checkpoint_dir / filename\n\n    # Prepare checkpoint dictionary\n    checkpoint = {\n        'epoch': epoch,\n        'iteration': iteration,\n        'student_state_dict': student.state_dict(),\n        'teacher_state_dict': teacher.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'dino_loss_center': dino_loss.center,\n        'config': config.to_dict() if hasattr(config, 'to_dict') else config,\n        'metrics': metrics or {},\n        'timestamp': datetime.now().isoformat(),\n        'has_history': history is not None,\n    }\n\n    # Save checkpoint\n    torch.save(checkpoint, checkpoint_path)\n    logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n\n    # Also save a \"latest\" checkpoint\n    latest_path = checkpoint_dir / 'checkpoint_latest.pth'\n    torch.save(checkpoint, latest_path)\n\n    # Save history to separate JSON file\n    if history is not None:\n        history_filename = checkpoint_path.stem + '_history.json'\n        history_path = checkpoint_dir / history_filename\n        history.save(history_path)\n\n        # Also save latest history\n        latest_history_path = checkpoint_dir / 'history_latest.json'\n        history.save(latest_history_path)\n\n    # Save best checkpoint if specified\n    if is_best:\n        best_path = checkpoint_dir / 'checkpoint_best.pth'\n        torch.save(checkpoint, best_path)\n        logger.info(f\"Best checkpoint saved to {best_path}\")\n\n    return checkpoint_path\n</code></pre>"},{"location":"api/utils/#load_checkpoint","title":"load_checkpoint","text":""},{"location":"api/utils/#dino.utils.load_checkpoint","title":"<code>load_checkpoint(checkpoint_path, student, teacher, optimizer, dino_loss, device='cpu')</code>","text":"<p>Load checkpoint and restore training state.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to checkpoint file</p> required <code>student</code> <code>Module</code> <p>Student model to load weights into</p> required <code>teacher</code> <code>Module</code> <p>Teacher model to load weights into</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer to load state into</p> required <code>dino_loss</code> <code>Module</code> <p>Loss module to restore center</p> required <code>device</code> <code>str</code> <p>Device to map checkpoint to</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with checkpoint info (epoch, iteration, metrics, etc.)</p> Source code in <code>src/dino/utils/checkpoint.py</code> <pre><code>def load_checkpoint(\n    checkpoint_path: str,\n    student: torch.nn.Module,\n    teacher: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    dino_loss: torch.nn.Module,\n    device: str = 'cpu'\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load checkpoint and restore training state.\n\n    Args:\n        checkpoint_path: Path to checkpoint file\n        student: Student model to load weights into\n        teacher: Teacher model to load weights into\n        optimizer: Optimizer to load state into\n        dino_loss: Loss module to restore center\n        device: Device to map checkpoint to\n\n    Returns:\n        Dictionary with checkpoint info (epoch, iteration, metrics, etc.)\n    \"\"\"\n    checkpoint_path = Path(checkpoint_path)\n\n    if not checkpoint_path.exists():\n        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\n    logger.info(f\"Loading checkpoint from {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n\n    # Load model states\n    student.load_state_dict(checkpoint['student_state_dict'])\n    teacher.load_state_dict(checkpoint['teacher_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    # Restore loss center (ensure it's on the correct device)\n    dino_loss.center = checkpoint['dino_loss_center'].to(device)\n\n    # Get training info\n    epoch = checkpoint['epoch']\n    iteration = checkpoint.get('iteration', 0)\n    metrics = checkpoint.get('metrics', {})\n\n    logger.info(\n        f\"Checkpoint loaded successfully! \"\n        f\"Epoch: {epoch}, Iteration: {iteration}\"\n    )\n\n    # Try to load history from associated JSON file\n    history = None\n    if checkpoint.get('has_history', False):\n        from .history import History\n        history_filename = checkpoint_path.stem + '_history.json'\n        history_path = checkpoint_path.parent / history_filename\n        if history_path.exists():\n            history = History.load(history_path)\n        else:\n            # Try loading latest history as fallback\n            latest_history_path = checkpoint_path.parent / 'history_latest.json'\n            if latest_history_path.exists():\n                history = History.load(latest_history_path)\n\n    return {\n        'epoch': epoch,\n        'iteration': iteration,\n        'metrics': metrics,\n        'config': checkpoint.get('config'),\n        'timestamp': checkpoint.get('timestamp'),\n        'history': history\n    }\n</code></pre>"},{"location":"api/utils/#history","title":"History","text":""},{"location":"api/utils/#dino.utils.History","title":"<code>History(metadata=None)</code>","text":"<p>Track training metrics at iteration and epoch granularity.</p> <p>Stores metrics in a structured format with JSON serialization support. Provides data access methods and basic matplotlib visualization.</p> <p>Core metrics tracked: - loss: Training loss value - learning_rate: Current learning rate from optimizer - momentum: Teacher momentum (EMA coefficient)</p> <p>Attributes:</p> Name Type Description <code>iteration_metrics</code> <code>List[Dict[str, Any]]</code> <p>List of dicts, one per logged iteration</p> <code>epoch_metrics</code> <code>List[Dict[str, Any]]</code> <p>List of dicts, one per epoch</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Dict with training run info (start_time, config, etc.)</p> Example <p>history = History() history.record_iteration(0, {'loss': 2.5, 'learning_rate': 0.001, 'momentum': 0.996}) history.record_epoch(1, {'loss': 2.3, 'learning_rate': 0.001, 'momentum': 0.997}) history.save('training_history.json') history.plot_loss()</p> <p>Initialize History tracker.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dict with run metadata (config, start_time, etc.)</p> <code>None</code> Source code in <code>src/dino/utils/history.py</code> <pre><code>def __init__(self, metadata: Optional[Dict[str, Any]] = None):\n    \"\"\"\n    Initialize History tracker.\n\n    Args:\n        metadata: Optional dict with run metadata (config, start_time, etc.)\n    \"\"\"\n    self.iteration_metrics: List[Dict[str, Any]] = []\n    self.epoch_metrics: List[Dict[str, Any]] = []\n    self.metadata: Dict[str, Any] = metadata or {}\n    self.metadata.setdefault('created_at', datetime.now().isoformat())\n</code></pre>"},{"location":"api/utils/#dino.utils.History.record_iteration","title":"<code>record_iteration(iteration, metrics)</code>","text":"<p>Record metrics for a single training iteration.</p> <p>Parameters:</p> Name Type Description Default <code>iteration</code> <code>int</code> <p>Global iteration number (0-indexed)</p> required <code>metrics</code> <code>Dict[str, float]</code> <p>Dict with metric names and values      Expected keys: 'loss', 'learning_rate', 'momentum'</p> required Source code in <code>src/dino/utils/history.py</code> <pre><code>def record_iteration(self, iteration: int, metrics: Dict[str, float]) -&gt; None:\n    \"\"\"\n    Record metrics for a single training iteration.\n\n    Args:\n        iteration: Global iteration number (0-indexed)\n        metrics: Dict with metric names and values\n                 Expected keys: 'loss', 'learning_rate', 'momentum'\n    \"\"\"\n    record = {\n        'iteration': iteration,\n        'timestamp': datetime.now().isoformat(),\n        **metrics\n    }\n    self.iteration_metrics.append(record)\n</code></pre>"},{"location":"api/utils/#dino.utils.History.record_epoch","title":"<code>record_epoch(epoch, metrics)</code>","text":"<p>Record summary metrics for a completed epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Epoch number (1-indexed, matching trainer convention)</p> required <code>metrics</code> <code>Dict[str, float]</code> <p>Dict with aggregated metric values      Expected keys: 'loss', 'learning_rate', 'momentum'</p> required Source code in <code>src/dino/utils/history.py</code> <pre><code>def record_epoch(self, epoch: int, metrics: Dict[str, float]) -&gt; None:\n    \"\"\"\n    Record summary metrics for a completed epoch.\n\n    Args:\n        epoch: Epoch number (1-indexed, matching trainer convention)\n        metrics: Dict with aggregated metric values\n                 Expected keys: 'loss', 'learning_rate', 'momentum'\n    \"\"\"\n    record = {\n        'epoch': epoch,\n        'timestamp': datetime.now().isoformat(),\n        **metrics\n    }\n    self.epoch_metrics.append(record)\n</code></pre>"},{"location":"api/utils/#dino.utils.History.get_metric","title":"<code>get_metric(name, level='iteration')</code>","text":"<p>Retrieve a specific metric as a list.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Metric name ('loss', 'learning_rate', 'momentum')</p> required <code>level</code> <code>str</code> <p>Either 'iteration' or 'epoch'</p> <code>'iteration'</code> <p>Returns:</p> Type Description <code>List[float]</code> <p>List of metric values in chronological order</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If level is invalid</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def get_metric(self, name: str, level: str = 'iteration') -&gt; List[float]:\n    \"\"\"\n    Retrieve a specific metric as a list.\n\n    Args:\n        name: Metric name ('loss', 'learning_rate', 'momentum')\n        level: Either 'iteration' or 'epoch'\n\n    Returns:\n        List of metric values in chronological order\n\n    Raises:\n        ValueError: If level is invalid\n    \"\"\"\n    if level == 'iteration':\n        data = self.iteration_metrics\n    elif level == 'epoch':\n        data = self.epoch_metrics\n    else:\n        raise ValueError(f\"Invalid level: {level}. Use 'iteration' or 'epoch'\")\n\n    return [record[name] for record in data if name in record]\n</code></pre>"},{"location":"api/utils/#dino.utils.History.get_iterations","title":"<code>get_iterations()</code>","text":"<p>Get list of recorded iteration numbers.</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def get_iterations(self) -&gt; List[int]:\n    \"\"\"Get list of recorded iteration numbers.\"\"\"\n    return [record['iteration'] for record in self.iteration_metrics]\n</code></pre>"},{"location":"api/utils/#dino.utils.History.get_epochs","title":"<code>get_epochs()</code>","text":"<p>Get list of recorded epoch numbers.</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def get_epochs(self) -&gt; List[int]:\n    \"\"\"Get list of recorded epoch numbers.\"\"\"\n    return [record['epoch'] for record in self.epoch_metrics]\n</code></pre>"},{"location":"api/utils/#dino.utils.History.save","title":"<code>save(path)</code>","text":"<p>Save history to JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>File path to save to (should end in .json)</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Path object to saved file</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def save(self, path: Union[str, Path]) -&gt; Path:\n    \"\"\"\n    Save history to JSON file.\n\n    Args:\n        path: File path to save to (should end in .json)\n\n    Returns:\n        Path object to saved file\n    \"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, 'w') as f:\n        json.dump(self.to_dict(), f, indent=2)\n\n    logger.info(f\"History saved to {path}\")\n    return path\n</code></pre>"},{"location":"api/utils/#dino.utils.History.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load history from JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to JSON file</p> required <p>Returns:</p> Type Description <code>History</code> <p>History instance with loaded data</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file does not exist</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>@classmethod\ndef load(cls, path: Union[str, Path]) -&gt; 'History':\n    \"\"\"\n    Load history from JSON file.\n\n    Args:\n        path: Path to JSON file\n\n    Returns:\n        History instance with loaded data\n\n    Raises:\n        FileNotFoundError: If file does not exist\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise FileNotFoundError(f\"History file not found: {path}\")\n\n    with open(path, 'r') as f:\n        data = json.load(f)\n\n    logger.info(f\"History loaded from {path}\")\n    return cls.from_dict(data)\n</code></pre>"},{"location":"api/utils/#dino.utils.History.plot_loss","title":"<code>plot_loss(level='epoch', ax=None, title='Training Loss', save_path=None, **kwargs)</code>","text":"<p>Plot training loss curve.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>'iteration' or 'epoch'</p> <code>'epoch'</code> <code>ax</code> <p>Optional matplotlib axes to plot on</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Training Loss'</code> <code>save_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to save figure</p> <code>None</code> <code>**kwargs</code> <p>Additional kwargs passed to plt.plot()</p> <code>{}</code> <p>Returns:</p> Type Description <p>matplotlib.axes.Axes object</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def plot_loss(\n    self,\n    level: str = 'epoch',\n    ax=None,\n    title: str = 'Training Loss',\n    save_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"\n    Plot training loss curve.\n\n    Args:\n        level: 'iteration' or 'epoch'\n        ax: Optional matplotlib axes to plot on\n        title: Plot title\n        save_path: Optional path to save figure\n        **kwargs: Additional kwargs passed to plt.plot()\n\n    Returns:\n        matplotlib.axes.Axes object\n    \"\"\"\n    return self._plot_metric(\n        name='loss',\n        level=level,\n        ax=ax,\n        title=title,\n        ylabel='Loss',\n        save_path=save_path,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/utils/#dino.utils.History.plot_learning_rate","title":"<code>plot_learning_rate(level='iteration', ax=None, title='Learning Rate Schedule', save_path=None, **kwargs)</code>","text":"<p>Plot learning rate over training.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>'iteration' or 'epoch'</p> <code>'iteration'</code> <code>ax</code> <p>Optional matplotlib axes</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Learning Rate Schedule'</code> <code>save_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to save figure</p> <code>None</code> <code>**kwargs</code> <p>Additional kwargs passed to plt.plot()</p> <code>{}</code> <p>Returns:</p> Type Description <p>matplotlib.axes.Axes object</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def plot_learning_rate(\n    self,\n    level: str = 'iteration',\n    ax=None,\n    title: str = 'Learning Rate Schedule',\n    save_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"\n    Plot learning rate over training.\n\n    Args:\n        level: 'iteration' or 'epoch'\n        ax: Optional matplotlib axes\n        title: Plot title\n        save_path: Optional path to save figure\n        **kwargs: Additional kwargs passed to plt.plot()\n\n    Returns:\n        matplotlib.axes.Axes object\n    \"\"\"\n    return self._plot_metric(\n        name='learning_rate',\n        level=level,\n        ax=ax,\n        title=title,\n        ylabel='Learning Rate',\n        save_path=save_path,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/utils/#dino.utils.History.plot_momentum","title":"<code>plot_momentum(level='iteration', ax=None, title='Teacher Momentum Schedule', save_path=None, **kwargs)</code>","text":"<p>Plot teacher momentum over training.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>'iteration' or 'epoch'</p> <code>'iteration'</code> <code>ax</code> <p>Optional matplotlib axes</p> <code>None</code> <code>title</code> <code>str</code> <p>Plot title</p> <code>'Teacher Momentum Schedule'</code> <code>save_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to save figure</p> <code>None</code> <code>**kwargs</code> <p>Additional kwargs passed to plt.plot()</p> <code>{}</code> <p>Returns:</p> Type Description <p>matplotlib.axes.Axes object</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def plot_momentum(\n    self,\n    level: str = 'iteration',\n    ax=None,\n    title: str = 'Teacher Momentum Schedule',\n    save_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"\n    Plot teacher momentum over training.\n\n    Args:\n        level: 'iteration' or 'epoch'\n        ax: Optional matplotlib axes\n        title: Plot title\n        save_path: Optional path to save figure\n        **kwargs: Additional kwargs passed to plt.plot()\n\n    Returns:\n        matplotlib.axes.Axes object\n    \"\"\"\n    return self._plot_metric(\n        name='momentum',\n        level=level,\n        ax=ax,\n        title=title,\n        ylabel='Momentum',\n        save_path=save_path,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/utils/#dino.utils.History.plot_all","title":"<code>plot_all(level='epoch', save_path=None)</code>","text":"<p>Create a figure with all metrics in subplots.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>'iteration' or 'epoch'</p> <code>'epoch'</code> <code>save_path</code> <code>Optional[Union[str, Path]]</code> <p>Optional path to save figure</p> <code>None</code> <p>Returns:</p> Type Description <p>matplotlib.figure.Figure object</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def plot_all(\n    self,\n    level: str = 'epoch',\n    save_path: Optional[Union[str, Path]] = None\n):\n    \"\"\"\n    Create a figure with all metrics in subplots.\n\n    Args:\n        level: 'iteration' or 'epoch'\n        save_path: Optional path to save figure\n\n    Returns:\n        matplotlib.figure.Figure object\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n    except ImportError:\n        raise ImportError(\n            \"matplotlib is required for plotting. \"\n            \"Install with: pip install matplotlib\"\n        )\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n    self.plot_loss(level=level, ax=axes[0])\n    self.plot_learning_rate(level=level, ax=axes[1])\n    self.plot_momentum(level=level, ax=axes[2])\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n        logger.info(f\"Combined plot saved to {save_path}\")\n\n    return fig\n</code></pre>"},{"location":"api/utils/#dino.utils.History.to_dataframe","title":"<code>to_dataframe(level='iteration')</code>","text":"<p>Convert history to pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Either 'iteration' or 'epoch'</p> <code>'iteration'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame with metrics</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If pandas is not installed</p> <code>ValueError</code> <p>If level is invalid</p> Source code in <code>src/dino/utils/history.py</code> <pre><code>def to_dataframe(self, level: str = 'iteration'):\n    \"\"\"\n    Convert history to pandas DataFrame.\n\n    Args:\n        level: Either 'iteration' or 'epoch'\n\n    Returns:\n        pandas.DataFrame with metrics\n\n    Raises:\n        ImportError: If pandas is not installed\n        ValueError: If level is invalid\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\n            \"pandas is required for to_dataframe(). \"\n            \"Install with: pip install pandas\"\n        )\n\n    if level == 'iteration':\n        return pd.DataFrame(self.iteration_metrics)\n    elif level == 'epoch':\n        return pd.DataFrame(self.epoch_metrics)\n    else:\n        raise ValueError(f\"Invalid level: {level}. Use 'iteration' or 'epoch'\")\n</code></pre>"},{"location":"api/utils/#usage-examples","title":"Usage Examples","text":""},{"location":"api/utils/#ema-updates","title":"EMA Updates","text":"<pre><code>from dino.utils import update_teacher_EMA, get_momentum_schedule\n\n# Simple EMA update\nupdate_teacher_EMA(student, teacher, momentum=0.996)\n\n# With momentum schedule\nschedule = get_momentum_schedule(\n    base=0.996,\n    final=1.0,\n    num_epochs=100,\n    niter_per_epoch=len(train_loader)\n)\n\nfor iteration in range(total_iterations):\n    # ... training step ...\n    momentum = schedule[iteration]\n    update_teacher_EMA(student, teacher, momentum)\n</code></pre>"},{"location":"api/utils/#saving-checkpoints","title":"Saving Checkpoints","text":"<pre><code>from dino.utils import save_checkpoint\n\nsave_checkpoint(\n    path='checkpoints/checkpoint_epoch_50.pth',\n    epoch=50,\n    iteration=10000,\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    config=config,\n    metrics={'loss': 2.5, 'knn_acc': 0.75}\n)\n</code></pre>"},{"location":"api/utils/#loading-checkpoints","title":"Loading Checkpoints","text":"<pre><code>from dino.utils import load_checkpoint\n\ncheckpoint = load_checkpoint(\n    path='checkpoints/checkpoint_latest.pth',\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    loss_fn=loss_fn\n)\n\n# Resume from checkpoint\nstart_epoch = checkpoint['epoch'] + 1\niteration = checkpoint['iteration']\n</code></pre>"},{"location":"api/utils/#training-history","title":"Training History","text":"<pre><code>from dino.utils import History\n\n# Create history tracker\nhistory = History(metadata={'config': config.to_dict()})\n\n# Record during training\nfor epoch in range(num_epochs):\n    for iteration, batch in enumerate(train_loader):\n        # ... training ...\n        history.record_iteration(total_iter, {\n            'loss': loss.item(),\n            'learning_rate': current_lr,\n            'momentum': current_momentum\n        })\n        total_iter += 1\n\n    history.record_epoch(epoch, {\n        'loss': epoch_loss,\n        'learning_rate': current_lr,\n        'momentum': current_momentum\n    })\n\n# Save history\nhistory.save('training_history.json')\n\n# Load history\nhistory = History.load('training_history.json')\n\n# Plot metrics\nhistory.plot_loss(level='epoch')\nhistory.plot_learning_rate(level='iteration')\nhistory.plot_all(level='epoch', save_path='training_plots.png')\n\n# Export to DataFrame\ndf = history.to_dataframe(level='epoch')\nprint(df.head())\n</code></pre>"},{"location":"api/utils/#getting-metrics","title":"Getting Metrics","text":"<pre><code># Get specific metric\nlosses = history.get_metric('loss', level='epoch')\nlearning_rates = history.get_metric('learning_rate', level='iteration')\n\n# Get indices\niterations = history.get_iterations()\nepochs = history.get_epochs()\n</code></pre>"},{"location":"getting-started/cli-reference/","title":"CLI Reference","text":"<p>Complete reference for command-line options.</p>"},{"location":"getting-started/cli-reference/#training-script","title":"Training Script","text":"<pre><code>python scripts/train.py [OPTIONS]\n</code></pre>"},{"location":"getting-started/cli-reference/#configuration","title":"Configuration","text":"Option Type Default Description <code>--config</code> path <code>configs/default.yaml</code> Path to configuration file <code>--resume</code> path None Resume from checkpoint"},{"location":"getting-started/cli-reference/#data-options","title":"Data Options","text":"Option Type Default Description <code>--batch-size</code> int 32 Batch size for training <code>--num-workers</code> int 4 Number of data loading workers"},{"location":"getting-started/cli-reference/#training-options","title":"Training Options","text":"Option Type Default Description <code>--epochs</code> int 100 Number of training epochs <code>--lr</code> float 0.001 Learning rate <code>--weight-decay</code> float 0.04 Weight decay for optimizer"},{"location":"getting-started/cli-reference/#model-options","title":"Model Options","text":"Option Type Default Description <code>--backbone</code> str resnet18 Backbone architecture"},{"location":"getting-started/cli-reference/#examples","title":"Examples","text":""},{"location":"getting-started/cli-reference/#basic-training","title":"Basic Training","text":"<pre><code># Train with defaults\npython scripts/train.py\n\n# Train with specific config\npython scripts/train.py --config configs/imagenet100.yaml\n</code></pre>"},{"location":"getting-started/cli-reference/#override-parameters","title":"Override Parameters","text":"<pre><code># Smaller batch size for limited GPU memory\npython scripts/train.py --batch-size 16\n\n# Longer training with lower learning rate\npython scripts/train.py --epochs 200 --lr 0.0005\n\n# Use ViT backbone\npython scripts/train.py --backbone dino_vits16\n</code></pre>"},{"location":"getting-started/cli-reference/#resume-training","title":"Resume Training","text":"<pre><code># Resume from latest checkpoint\npython scripts/train.py --resume checkpoints/checkpoint_latest.pth\n\n# Resume with modified parameters\npython scripts/train.py --resume checkpoints/checkpoint_epoch_50.pth --lr 0.0001\n</code></pre>"},{"location":"getting-started/cli-reference/#multiple-overrides","title":"Multiple Overrides","text":"<pre><code>python scripts/train.py \\\n    --config configs/imagenet100.yaml \\\n    --batch-size 64 \\\n    --epochs 200 \\\n    --lr 0.0005 \\\n    --num-workers 8\n</code></pre>"},{"location":"getting-started/cli-reference/#priority-order","title":"Priority Order","text":"<p>Command-line arguments override configuration file values:</p> <ol> <li>CLI arguments (highest priority)</li> <li>YAML configuration file</li> <li>Default values (lowest priority)</li> </ol> <p>Example: <pre><code># config.yaml has batch_size: 32\n# CLI overrides to 64\npython scripts/train.py --config config.yaml --batch-size 64\n# Result: batch_size = 64\n</code></pre></p>"},{"location":"getting-started/cli-reference/#environment-variables","title":"Environment Variables","text":"Variable Description <code>CUDA_VISIBLE_DEVICES</code> GPUs to use (e.g., \"0,1\") <code>TORCH_HOME</code> PyTorch cache directory <p>Example: <pre><code># Use only GPU 0\nCUDA_VISIBLE_DEVICES=0 python scripts/train.py\n\n# Use GPUs 0 and 1\nCUDA_VISIBLE_DEVICES=0,1 python scripts/train.py\n</code></pre></p>"},{"location":"getting-started/cli-reference/#see-also","title":"See Also","text":"<ul> <li>Configuration - Full YAML configuration reference</li> <li>Training - Training pipeline details</li> <li>Checkpointing - Checkpoint management</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers the installation of DINO and its dependencies.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>CUDA (optional, for GPU training)</li> </ul>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":""},{"location":"getting-started/installation/#using-uv-recommended","title":"Using uv (Recommended)","text":"<p>uv is a fast, modern Python package manager.</p> <pre><code># Clone the repository\ngit clone &lt;your-repo-url&gt;\ncd DinoImpl\n\n# Install with uv\nuv sync\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<pre><code># Clone the repository\ngit clone &lt;your-repo-url&gt;\ncd DinoImpl\n\n# Install in development mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<p>To build the documentation locally:</p> <pre><code>pip install -e \".[docs]\"\n# or\nuv pip install -e \".[docs]\"\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For linting, testing, and code formatting:</p> <pre><code>pip install -e \".[dev]\"\n# or\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#weights-biases","title":"Weights &amp; Biases","text":"<p>For experiment tracking with W&amp;B:</p> <pre><code>pip install -e \".[wandb]\"\n</code></pre>"},{"location":"getting-started/installation/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>For running notebooks:</p> <pre><code>pip install -e \".[notebooks]\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify everything works:</p> <pre><code># Check the module is importable\npython -c \"from dino import DinoConfig, DinoModel; print('Installation successful!')\"\n\n# Check available commands\npython scripts/train.py --help\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup","title":"GPU Setup","text":"<p>DINO benefits significantly from GPU acceleration. Ensure your PyTorch installation includes CUDA support:</p> <pre><code># Check CUDA availability\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\npython -c \"import torch; print(f'CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"None\\\"}')\"\n</code></pre> <p>If CUDA is not available, reinstall PyTorch with CUDA support from pytorch.org.</p>"},{"location":"getting-started/installation/#dataset-setup","title":"Dataset Setup","text":""},{"location":"getting-started/installation/#imagenette-default","title":"ImageNette (Default)","text":"<p>ImageNette downloads automatically when you first run training:</p> <pre><code>python scripts/train.py\n# Dataset will be downloaded to ./data/imagenette2/\n</code></pre>"},{"location":"getting-started/installation/#imagenet100","title":"ImageNet100","text":"<p>ImageNet100 requires manual download from Kaggle:</p> <pre><code># Install Kaggle CLI\npip install kaggle\n\n# Configure Kaggle API (requires account)\n# Place kaggle.json in ~/.kaggle/\n\n# Download dataset\nkaggle datasets download -d ambityga/imagenet100\n\n# Extract to data directory\nunzip imagenet100.zip -d ./data/imagenet100\n</code></pre> <p>See Data Pipeline for more dataset options.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Train your first model</li> <li>CLI Reference - Command-line options</li> <li>Configuration - Full configuration guide</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with DINO in 5 minutes.</p>"},{"location":"getting-started/quickstart/#train-your-first-model","title":"Train Your First Model","text":""},{"location":"getting-started/quickstart/#basic-training","title":"Basic Training","text":"<pre><code># Train on ImageNette (default dataset)\npython scripts/train.py\n</code></pre> <p>That's it! Your model will train and save checkpoints to <code>./checkpoints/</code>.</p>"},{"location":"getting-started/quickstart/#training-with-different-datasets","title":"Training with Different Datasets","text":"<pre><code># Train on ImageNet100\npython scripts/train.py --config configs/imagenet100.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#custom-settings","title":"Custom Settings","text":"<pre><code># Override specific parameters\npython scripts/train.py --epochs 100 --batch-size 64 --lr 0.001\n</code></pre>"},{"location":"getting-started/quickstart/#training-options","title":"Training Options","text":""},{"location":"getting-started/quickstart/#with-configuration-file","title":"With Configuration File","text":"<pre><code>python scripts/train.py --config configs/default.yaml\n</code></pre>"},{"location":"getting-started/quickstart/#override-parameters","title":"Override Parameters","text":"<pre><code>python scripts/train.py \\\n    --config configs/imagenet100.yaml \\\n    --batch-size 64 \\\n    --epochs 200 \\\n    --lr 0.0005\n</code></pre>"},{"location":"getting-started/quickstart/#resume-from-checkpoint","title":"Resume from Checkpoint","text":"<pre><code>python scripts/train.py --resume checkpoints/checkpoint_latest.pth\n</code></pre>"},{"location":"getting-started/quickstart/#using-dino-in-python","title":"Using DINO in Python","text":""},{"location":"getting-started/quickstart/#simple-usage-with-factory-methods","title":"Simple Usage with Factory Methods","text":"<pre><code>from dino.config import DinoConfig\nfrom dino.models import DinoModel\nfrom dino.loss import DinoLoss\nfrom dino.training import DinoTrainer, create_optimizer, create_scheduler\nfrom dino.data import create_dataloaders\n\n# Load configuration\nconfig = DinoConfig.from_yaml('configs/default.yaml')\n\n# Create components using factory methods\ntrain_loader, val_loader, _ = create_dataloaders(config)\nstudent = DinoModel.from_config(config)\nteacher = DinoModel.from_config(config)\nteacher.load_state_dict(student.state_dict())\n\n# Create loss and optimizer\nloss_fn = DinoLoss.from_config(config.loss, config.augmentation, out_dim=student.output_dim)\noptimizer = create_optimizer(student.parameters(), config.optimizer)\n\n# Train\ntrainer = DinoTrainer(\n    config=config,\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    scheduler=None,\n    loss_fn=loss_fn,\n    train_loader=train_loader\n)\ntrainer.train()\n</code></pre>"},{"location":"getting-started/quickstart/#manual-component-creation","title":"Manual Component Creation","text":"<p>For more control over the components:</p> <pre><code>from dino.models import get_backbone, DinoProjectionHead, DinoModel\n\n# Create components manually\nbackbone = get_backbone('resnet18')\nprojection = DinoProjectionHead(input_dim=512, output_dim=2048)\nstudent = DinoModel(backbone, projection)\n</code></pre>"},{"location":"getting-started/quickstart/#monitoring-training","title":"Monitoring Training","text":""},{"location":"getting-started/quickstart/#console-output","title":"Console Output","text":"<pre><code>[INFO] Epoch 1/100\nEpoch 1/100: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 208/208 [02:15&lt;00:00]  loss: 8.2341, momentum: 0.9960\n[INFO] Train Epoch 1 - loss: 8.2145, momentum: 0.9960\n</code></pre>"},{"location":"getting-started/quickstart/#tensorboard","title":"TensorBoard","text":"<pre><code>tensorboard --logdir logs/\n</code></pre>"},{"location":"getting-started/quickstart/#check-checkpoints","title":"Check Checkpoints","text":"<pre><code>ls -lh checkpoints/\n</code></pre>"},{"location":"getting-started/quickstart/#training-history","title":"Training History","text":"<p>Track and visualize training metrics:</p> <pre><code>from dino.utils import History\n\n# Load history from a training run\nhistory = History.load('training_history.json')\n\n# Plot metrics\nhistory.plot_loss(level='epoch')\nhistory.plot_learning_rate(level='iteration')\nhistory.plot_all(level='epoch', save_path='training_plots.png')\n\n# Export to DataFrame (requires pandas)\ndf = history.to_dataframe(level='epoch')\nprint(df.head())\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>CLI Reference - All command-line options</li> <li>Configuration - Customize your training</li> <li>Models - Available backbones and architectures</li> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"guides/checkpointing/","title":"Checkpointing","text":"<p>Guide to saving, loading, and resuming training from checkpoints.</p>"},{"location":"guides/checkpointing/#overview","title":"Overview","text":"<p>Checkpoints save the complete training state, allowing you to:</p> <ul> <li>Resume training after interruption</li> <li>Evaluate models at specific epochs</li> <li>Share trained models</li> </ul>"},{"location":"guides/checkpointing/#checkpoint-contents","title":"Checkpoint Contents","text":"<p>Each checkpoint includes:</p> <pre><code>checkpoint = {\n    'epoch': int,                    # Current epoch\n    'iteration': int,                # Current iteration\n    'student_state_dict': OrderedDict,   # Student weights\n    'teacher_state_dict': OrderedDict,   # Teacher weights\n    'optimizer_state_dict': dict,    # Optimizer state\n    'dino_loss_center': Tensor,      # Loss centering buffer\n    'config': dict,                  # Training configuration\n    'metrics': dict,                 # Training metrics\n    'timestamp': str                 # Save time\n}\n</code></pre>"},{"location":"guides/checkpointing/#why-save-the-loss-center","title":"Why Save the Loss Center?","text":"<p>The DINO loss maintains a running center via EMA. This center is critical for:</p> <ul> <li>Stable loss computation</li> <li>Preventing collapse</li> <li>Consistent training resumption</li> </ul> <p>Losing the center would break loss computation when resuming.</p>"},{"location":"guides/checkpointing/#automatic-saving","title":"Automatic Saving","text":"<p>Checkpoints are saved automatically during training:</p> <pre><code>checkpoint:\n  save_dir: ./checkpoints     # Where to save\n  save_freq: 10               # Save every N epochs\n  save_latest: true           # Keep checkpoint_latest.pth\n  save_best: true             # Keep checkpoint_best.pth\n</code></pre>"},{"location":"guides/checkpointing/#files-created","title":"Files Created","text":"<ul> <li><code>checkpoint_epoch_XXXX.pth</code> - Checkpoint for specific epoch</li> <li><code>checkpoint_latest.pth</code> - Most recent checkpoint</li> <li><code>checkpoint_best.pth</code> - Best performing checkpoint (if enabled)</li> </ul>"},{"location":"guides/checkpointing/#resuming-training","title":"Resuming Training","text":""},{"location":"guides/checkpointing/#from-command-line","title":"From Command Line","text":"<pre><code># Resume from latest\npython scripts/train.py --resume checkpoints/checkpoint_latest.pth\n\n# Resume from specific epoch\npython scripts/train.py --resume checkpoints/checkpoint_epoch_0050.pth\n</code></pre>"},{"location":"guides/checkpointing/#from-python","title":"From Python","text":"<pre><code>from dino.utils import load_checkpoint\n\n# Load checkpoint\ncheckpoint = load_checkpoint(\n    'checkpoints/checkpoint_latest.pth',\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    loss_fn=loss_fn\n)\n\n# Get resume information\nstart_epoch = checkpoint['epoch'] + 1\niteration = checkpoint['iteration']\n</code></pre>"},{"location":"guides/checkpointing/#manual-saving","title":"Manual Saving","text":"<pre><code>from dino.utils import save_checkpoint\n\nsave_checkpoint(\n    path='checkpoints/my_checkpoint.pth',\n    epoch=50,\n    iteration=10000,\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    loss_fn=loss_fn,\n    config=config,\n    metrics={'loss': 2.5}\n)\n</code></pre>"},{"location":"guides/checkpointing/#loading-for-evaluation","title":"Loading for Evaluation","text":"<p>When loading for evaluation (not training), you only need the model weights:</p> <pre><code>import torch\nfrom dino.models import DinoModel\n\n# Load checkpoint\ncheckpoint = torch.load('checkpoints/checkpoint_best.pth')\n\n# Create model\nmodel = DinoModel.from_config(config)\n\n# Load weights (student or teacher)\nmodel.load_state_dict(checkpoint['student_state_dict'])\n# or for evaluation, teacher often performs better:\nmodel.load_state_dict(checkpoint['teacher_state_dict'])\n\nmodel.eval()\n</code></pre>"},{"location":"guides/checkpointing/#extracting-features","title":"Extracting Features","text":"<pre><code># Load trained model\ncheckpoint = torch.load('checkpoints/checkpoint_best.pth')\nmodel = DinoModel.from_config(config)\nmodel.load_state_dict(checkpoint['teacher_state_dict'])\nmodel.eval()\n\n# Extract features\nwith torch.no_grad():\n    # Get backbone features (for downstream tasks)\n    features, projections = model(images, return_backbone_features=True)\n\n    # Features shape: [batch, backbone_dim] (e.g., 512 for ResNet18)\n    # Projections shape: [batch, projection_dim] (e.g., 2048)\n</code></pre>"},{"location":"guides/checkpointing/#checkpoint-compatibility","title":"Checkpoint Compatibility","text":"<p>Version Compatibility</p> <p>Checkpoints are not guaranteed to be compatible across different code versions. If you've made significant changes to model architecture, you may need to retrain from scratch.</p>"},{"location":"guides/checkpointing/#common-issues","title":"Common Issues","text":"<p>Missing keys: <pre><code># Strict loading (fails on mismatch)\nmodel.load_state_dict(checkpoint['state_dict'], strict=True)\n\n# Lenient loading (ignores mismatches)\nmodel.load_state_dict(checkpoint['state_dict'], strict=False)\n</code></pre></p> <p>Architecture changes: - If you change backbone or projection dimensions, old checkpoints won't work - Keep track of which config was used for each checkpoint</p>"},{"location":"guides/checkpointing/#best-practices","title":"Best Practices","text":"<ol> <li>Always save config: Include the configuration used for training</li> <li>Version your checkpoints: Use descriptive names or directories</li> <li>Save both student and teacher: Teacher often performs better for evaluation</li> <li>Save regularly: Prevent loss from crashes or interruptions</li> <li>Keep best checkpoint: Enable <code>save_best: true</code> in config</li> </ol>"},{"location":"guides/checkpointing/#configuration","title":"Configuration","text":"<pre><code>checkpoint:\n  save_dir: ./checkpoints     # Checkpoint directory\n  save_freq: 10               # Save every N epochs\n  save_latest: true           # Always keep latest\n  save_best: true             # Keep best performing\n</code></pre>"},{"location":"guides/checkpointing/#see-also","title":"See Also","text":"<ul> <li>Training - Training loop details</li> <li>CLI Reference - Resume options</li> <li>API Reference: Utils - Checkpoint functions</li> </ul>"},{"location":"guides/configuration/","title":"Configuration","text":"<p>Complete guide to the YAML configuration system.</p>"},{"location":"guides/configuration/#overview","title":"Overview","text":"<p>All settings are managed through YAML configuration files using Python dataclasses for type safety.</p> <pre><code># configs/default.yaml\ndata:\n  dataset: imagenette\n  batch_size: 32\n\nmodel:\n  backbone: resnet18\n\ntraining:\n  num_epochs: 100\n</code></pre>"},{"location":"guides/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>DinoConfig\n\u251c\u2500\u2500 DataConfig           # Dataset, batch size, splits\n\u251c\u2500\u2500 AugmentationConfig   # Crop sizes, color jitter, etc.\n\u251c\u2500\u2500 ModelConfig          # Backbone, projection dims\n\u251c\u2500\u2500 LossConfig           # Temperatures, center momentum\n\u251c\u2500\u2500 OptimizerConfig      # Learning rate, weight decay\n\u251c\u2500\u2500 SchedulerConfig      # LR scheduler configuration\n\u251c\u2500\u2500 TrainingConfig       # Epochs, teacher momentum\n\u251c\u2500\u2500 CheckpointConfig     # Save frequency, directory\n\u2514\u2500\u2500 LoggingConfig        # Log directory, TensorBoard\n</code></pre>"},{"location":"guides/configuration/#loading-configuration","title":"Loading Configuration","text":""},{"location":"guides/configuration/#from-yaml","title":"From YAML","text":"<pre><code>from dino.config import DinoConfig\n\nconfig = DinoConfig.from_yaml('configs/default.yaml')\n</code></pre>"},{"location":"guides/configuration/#cli-overrides","title":"CLI Overrides","text":"<pre><code># Load YAML, then override\nconfig = DinoConfig.from_yaml('config.yaml')\nif args.batch_size:\n    config.data.batch_size = args.batch_size\nif args.lr:\n    config.optimizer.lr = args.lr\n</code></pre> <p>Priority: CLI args &gt; YAML file &gt; Defaults</p>"},{"location":"guides/configuration/#configuration-sections","title":"Configuration Sections","text":""},{"location":"guides/configuration/#dataconfig","title":"DataConfig","text":"<pre><code>data:\n  dataset: imagenette         # Dataset name (imagenette, imagenet100)\n  data_path: ./data           # Root path for datasets\n  batch_size: 32              # Training batch size\n  num_workers: 4              # DataLoader workers\n  pin_memory: true            # Pin memory for GPU\n  train_split: 0.9            # Train/val split ratio\n</code></pre>"},{"location":"guides/configuration/#augmentationconfig","title":"AugmentationConfig","text":"<pre><code>augmentation:\n  # Crop settings\n  n_global_crops: 2           # Number of global crops\n  num_local_views: 6          # Number of local crops\n  global_crop_size: 224       # Global crop size (pixels)\n  local_crop_size: 96         # Local crop size (pixels)\n  global_crop_scale: [0.4, 1.0]   # Scale range for global crops\n  local_crop_scale: [0.05, 0.4]   # Scale range for local crops\n\n  # Color augmentation\n  color_jitter_prob: 0.8      # Probability of color jitter\n  brightness: 0.4\n  contrast: 0.4\n  saturation: 0.2\n  hue: 0.1\n\n  # Other augmentations\n  gaussian_blur_prob: 0.5     # Probability of Gaussian blur\n  solarization_prob: 0.2      # Probability of solarization\n\n  # Normalization (ImageNet defaults)\n  normalize_mean: [0.485, 0.456, 0.406]\n  normalize_std: [0.229, 0.224, 0.225]\n</code></pre>"},{"location":"guides/configuration/#modelconfig","title":"ModelConfig","text":"<pre><code>model:\n  # Backbone\n  backbone: resnet18          # resnet18/34/50/101/152, dino_vits8/16, dino_vitb8/16\n  pretrained_backbone: false  # Use pretrained weights\n\n  # Projection head\n  projection_hidden_dim: 1024\n  projection_bottleneck_dim: 256\n  projection_output_dim: 2048\n  projection_use_bn: false    # Batch normalization in projection\n</code></pre>"},{"location":"guides/configuration/#lossconfig","title":"LossConfig","text":"<pre><code>loss:\n  student_temp: 0.1           # Student temperature (higher = softer)\n  teacher_temp: 0.04          # Teacher temperature (lower = sharper)\n  center_momentum: 0.9        # EMA momentum for centering\n</code></pre>"},{"location":"guides/configuration/#optimizerconfig","title":"OptimizerConfig","text":"<pre><code>optimizer:\n  optimizer: adamw            # Optimizer type\n  lr: 0.001                   # Learning rate\n  weight_decay: 0.04          # Weight decay\n  betas: [0.9, 0.999]         # Adam betas\n</code></pre>"},{"location":"guides/configuration/#schedulerconfig","title":"SchedulerConfig","text":"<pre><code>scheduler:\n  scheduler: cosine_warmup    # Scheduler type\n  warmup_epochs: 10           # Warmup epochs\n  min_lr: 1.0e-6              # Minimum learning rate\n  warmup_start_lr: 0.0        # Starting LR for warmup\n</code></pre>"},{"location":"guides/configuration/#trainingconfig","title":"TrainingConfig","text":"<pre><code>training:\n  num_epochs: 100             # Total epochs\n  teacher_momentum: 0.996     # EMA momentum for teacher\n  teacher_momentum_final: 1.0 # Final momentum (if scheduled)\n  use_momentum_schedule: true # Use momentum scheduling\n  gradient_clip: 3.0          # Gradient clipping (null to disable)\n</code></pre>"},{"location":"guides/configuration/#checkpointconfig","title":"CheckpointConfig","text":"<pre><code>checkpoint:\n  save_dir: ./checkpoints     # Checkpoint directory\n  save_freq: 10               # Save every N epochs\n  save_latest: true           # Keep latest checkpoint\n  save_best: true             # Keep best checkpoint\n</code></pre>"},{"location":"guides/configuration/#loggingconfig","title":"LoggingConfig","text":"<pre><code>logging:\n  log_dir: ./logs             # Log directory\n  log_freq: 100               # Log every N iterations\n  use_tensorboard: true       # Enable TensorBoard\n  log_level: INFO             # Logging level\n</code></pre>"},{"location":"guides/configuration/#complete-example","title":"Complete Example","text":"<pre><code># configs/imagenet100.yaml\ndata:\n  dataset: imagenet100\n  data_path: ./data/imagenet100\n  batch_size: 64\n  num_workers: 8\n  pin_memory: true\n\naugmentation:\n  n_global_crops: 2\n  num_local_views: 6\n  global_crop_size: 224\n  local_crop_size: 96\n\nmodel:\n  backbone: resnet50\n  pretrained_backbone: false\n  projection_output_dim: 2048\n\nloss:\n  student_temp: 0.1\n  teacher_temp: 0.04\n  center_momentum: 0.9\n\noptimizer:\n  optimizer: adamw\n  lr: 0.0005\n  weight_decay: 0.04\n\nscheduler:\n  scheduler: cosine_warmup\n  warmup_epochs: 10\n  min_lr: 1.0e-6\n\ntraining:\n  num_epochs: 200\n  teacher_momentum: 0.996\n  gradient_clip: 3.0\n\ncheckpoint:\n  save_dir: ./checkpoints\n  save_freq: 10\n\nlogging:\n  log_dir: ./logs\n  use_tensorboard: true\n</code></pre>"},{"location":"guides/configuration/#validation","title":"Validation","text":"<p>Configuration is validated at multiple levels:</p> <ol> <li>Type checking: Dataclass enforces types</li> <li>Value checking: In <code>__post_init__</code> methods</li> <li>Runtime checking: In component constructors</li> </ol> <p>Example validation:</p> <pre><code>@dataclass\nclass LossConfig:\n    student_temp: float = 0.1\n    teacher_temp: float = 0.04\n\n    def __post_init__(self):\n        if self.student_temp &lt;= 0:\n            raise ValueError(\"Temperature must be positive\")\n        if self.student_temp &lt;= self.teacher_temp:\n            warnings.warn(\"Student temp should be &gt; teacher temp\")\n</code></pre>"},{"location":"guides/configuration/#using-factory-methods","title":"Using Factory Methods","text":"<p>Components can be created directly from config:</p> <pre><code># Models\nmodel = DinoModel.from_config(config)\n\n# Loss\nloss_fn = DinoLoss.from_config(config.loss, config.augmentation, out_dim)\n\n# Transforms\ntransform = DINOTransform.from_config(config.augmentation)\n\n# Optimizer and Scheduler\noptimizer = create_optimizer(model.parameters(), config.optimizer)\nscheduler = create_scheduler(optimizer, config.scheduler, config.optimizer, total_steps, warmup_steps)\n</code></pre>"},{"location":"guides/configuration/#see-also","title":"See Also","text":"<ul> <li>CLI Reference - Command-line options</li> <li>Training - Training configuration details</li> <li>API Reference - Component API documentation</li> </ul>"},{"location":"guides/data-pipeline/","title":"Data Pipeline","text":"<p>This guide explains the DINO data pipeline: augmentations, datasets, and dataloaders.</p>"},{"location":"guides/data-pipeline/#overview","title":"Overview","text":"<p>The data pipeline follows this flow:</p> <pre><code>Image Dataset\n    \u2193\nDINOTransform (multi-crop augmentation)\n    \u2193\nDataLoader (batch + custom collate)\n    \u2193\n[8 views per image: 2 global + 6 local]\n</code></pre>"},{"location":"guides/data-pipeline/#multi-crop-transform","title":"Multi-Crop Transform","text":"<p>The <code>DINOTransform</code> class creates multiple augmented views of each image.</p>"},{"location":"guides/data-pipeline/#architecture","title":"Architecture","text":"<pre><code>class DINOTransform:\n    def __init__(self, num_local_views=6, ...):\n        # Global view 1: Standard augmentation\n        self.global_t1 = Compose([\n            RandomResizedCrop(224, scale=(0.4, 1.0)),\n            ColorJitter + GaussianBlur\n        ])\n\n        # Global view 2: Add solarization\n        self.global_t2 = Compose([\n            ..., RandomSolarize(...)\n        ])\n\n        # Local views: Smaller crops\n        self.local_transform = Compose([\n            RandomResizedCrop(96, scale=(0.05, 0.4)),\n            ...\n        ])\n\n    def __call__(self, img) -&gt; List[Tensor]:\n        return [\n            self.global_t1(img),\n            self.global_t2(img),\n            *[self.local_transform(img) for _ in range(6)]\n        ]\n</code></pre>"},{"location":"guides/data-pipeline/#view-types","title":"View Types","text":"View Type Count Crop Size Scale Range Special Global 1 1 224\u00d7224 0.4 - 1.0 Standard augmentation Global 2 1 224\u00d7224 0.4 - 1.0 + Solarization Local 6 96\u00d796 0.05 - 0.4 Aggressive cropping"},{"location":"guides/data-pipeline/#augmentation-details","title":"Augmentation Details","text":"<p>Color Jitter: - Brightness: 0.4 - Contrast: 0.4 - Saturation: 0.2 - Hue: 0.1</p> <p>Gaussian Blur: - Applied with probability 0.5 (global views) - Kernel size proportional to crop size</p> <p>Solarization: - Applied only to global view 2 - Probability: 0.2</p>"},{"location":"guides/data-pipeline/#using-dinotransform","title":"Using DINOTransform","text":"<pre><code>from dino.data import DINOTransform\n\n# From config\ntransform = DINOTransform.from_config(config.augmentation)\n\n# Manual creation\ntransform = DINOTransform(\n    num_local_views=6,\n    global_crop_size=224,\n    local_crop_size=96,\n    global_crop_scale=(0.4, 1.0),\n    local_crop_scale=(0.05, 0.4)\n)\n\n# Apply to image\nviews = transform(image)  # Returns list of 8 tensors\n</code></pre>"},{"location":"guides/data-pipeline/#datasets","title":"Datasets","text":""},{"location":"guides/data-pipeline/#supported-datasets","title":"Supported Datasets","text":""},{"location":"guides/data-pipeline/#imagenette-default","title":"ImageNette (Default)","text":"<ul> <li>Description: 10-class subset of ImageNet</li> <li>Size: ~9,500 images</li> <li>Resolution: 224\u00d7224</li> <li>Download: Automatic via torchvision</li> </ul> <pre><code># configs/default.yaml\ndata:\n  dataset: imagenette\n  data_path: ./data\n</code></pre>"},{"location":"guides/data-pipeline/#imagenet100","title":"ImageNet100","text":"<ul> <li>Description: 100-class subset of ImageNet (from Kaggle)</li> <li>Size: ~130,000 training images</li> <li>Resolution: 224\u00d7224</li> <li>Download: Manual (Kaggle)</li> </ul> <pre><code># configs/imagenet100.yaml\ndata:\n  dataset: imagenet100\n  data_path: ./data/imagenet100\n</code></pre> <p>Setup ImageNet100:</p> <pre><code># Install Kaggle CLI\npip install kaggle\n\n# Download dataset (requires Kaggle account)\nkaggle datasets download -d ambityga/imagenet100\n\n# Extract to data directory\nunzip imagenet100.zip -d ./data/imagenet100\n</code></pre> <p>The dataset has a multi-folder structure (<code>train.X1</code>, <code>train.X2</code>, <code>train.X3</code>, <code>train.X4</code>, <code>val.X</code>) that is handled automatically via <code>ConcatDataset</code>.</p>"},{"location":"guides/data-pipeline/#factory-function","title":"Factory Function","text":"<pre><code>from dino.data import get_dataset\n\n# Get dataset with transform\ndataset = get_dataset(\n    name='imagenette',\n    path='./data',\n    transform=transform,\n    split='train'\n)\n</code></pre>"},{"location":"guides/data-pipeline/#dataloaders","title":"DataLoaders","text":""},{"location":"guides/data-pipeline/#creating-dataloaders","title":"Creating DataLoaders","text":"<pre><code>from dino.data import create_dataloaders\n\n# Create all dataloaders from config\ntrain_loader, val_loader, test_loader = create_dataloaders(config)\n</code></pre>"},{"location":"guides/data-pipeline/#custom-collate-function","title":"Custom Collate Function","text":"<p>Multi-crop transforms return a list of tensors, requiring a custom collate function:</p> <pre><code>def collate_multi_crop(batch):\n    \"\"\"Transpose list-of-lists to list of batched tensors.\"\"\"\n    views_lists = [item[0] for item in batch]  # Each: [g1, g2, l1, ..., l6]\n    labels = [item[1] for item in batch]\n\n    # Transpose: [[g1_img1, g1_img2, ...], [g2_img1, g2_img2, ...], ...]\n    views_batch = [\n        torch.stack([views[i] for views in views_lists])\n        for i in range(num_views)\n    ]\n    return views_batch, torch.tensor(labels)\n</code></pre> <p>Why custom collate?</p> <ul> <li>Multi-crop returns list of tensors, not single tensor</li> <li>Need to group by view type (all global1 together, all global2 together, etc.)</li> </ul>"},{"location":"guides/data-pipeline/#dataloader-configuration","title":"DataLoader Configuration","text":"<pre><code>data:\n  batch_size: 32\n  num_workers: 4\n  pin_memory: true\n</code></pre>"},{"location":"guides/data-pipeline/#tensor-shapes","title":"Tensor Shapes","text":"Stage Shape Notes Original image <code>[3, H, W]</code> Single RGB image After DINOTransform <code>List[8 tensors]</code> Each: <code>[3, crop_size, crop_size]</code> After collate <code>List[8 batches]</code> Each: <code>[batch, 3, crop_size, crop_size]</code> Global views <code>[batch, 3, 224, 224]</code> 2 views Local views <code>[batch, 3, 96, 96]</code> 6 views"},{"location":"guides/data-pipeline/#adding-custom-datasets","title":"Adding Custom Datasets","text":"<ol> <li>Add to <code>get_dataset()</code> in <code>src/dino/data/datasets.py</code>:</li> </ol> <pre><code>elif dataset_name == 'custom_dataset':\n    return torchvision.datasets.ImageFolder(\n        root=os.path.join(data_path, 'custom'),\n        transform=transform\n    )\n</code></pre> <ol> <li>Create config file <code>configs/custom_dataset.yaml</code>:</li> </ol> <pre><code>data:\n  dataset: custom_dataset\n  data_path: ./data\n  batch_size: 32\n</code></pre> <ol> <li>Run training:</li> </ol> <pre><code>python scripts/train.py --config configs/custom_dataset.yaml\n</code></pre>"},{"location":"guides/data-pipeline/#performance-tips","title":"Performance Tips","text":"<ul> <li>num_workers: Set to number of CPU cores</li> <li>pin_memory: Enable for GPU training</li> <li>persistent_workers: Keep workers alive between epochs</li> </ul> <pre><code>data:\n  num_workers: 8\n  pin_memory: true\n</code></pre>"},{"location":"guides/data-pipeline/#see-also","title":"See Also","text":"<ul> <li>Configuration - Full configuration reference</li> <li>Models - Backbone architectures</li> <li>API Reference: Data - API documentation</li> </ul>"},{"location":"guides/loss-function/","title":"Loss Function","text":"<p>This guide explains the DINO loss function, including temperature scaling, centering, and the cross-entropy computation.</p>"},{"location":"guides/loss-function/#overview","title":"Overview","text":"<p>The DINO loss encourages the student to match the teacher's predictions across different views of the same image.</p> <pre><code>Student (all 8 views) \u2192 Softmax \u2192 \u2500\u2500\u2500\u2500\u2500\u2510\n                                       \u251c\u2192 Cross-Entropy Loss\nTeacher (2 global views) \u2192 Softmax \u2192 \u2500\u2500\u2518\n</code></pre>"},{"location":"guides/loss-function/#core-computation","title":"Core Computation","text":"<pre><code>def forward(self, student_outputs, teacher_outputs):\n    # 1. Temperature scaling\n    student_logits = student_outputs / student_temp  # Sharper\n    teacher_logits = (teacher_outputs - center) / teacher_temp  # Softer\n\n    # 2. Probability distributions\n    student_log_probs = log_softmax(student_logits)\n    teacher_probs = softmax(teacher_logits).detach()  # No gradients\n\n    # 3. Chunk into views\n    student_views = student_log_probs.chunk(8)  # All views\n    teacher_views = teacher_probs.chunk(2)      # Global only\n\n    # 4. Cross-entropy between all pairs (except same view)\n    for i, teacher_view in enumerate(teacher_views):\n        for j, student_view in enumerate(student_views):\n            if i == j: continue  # Skip same view\n            loss += -sum(teacher_view * student_view)\n\n    # 5. Update center with EMA\n    center = momentum * center + (1 - momentum) * teacher_outputs.mean()\n\n    return loss / num_pairs\n</code></pre>"},{"location":"guides/loss-function/#key-mechanisms","title":"Key Mechanisms","text":""},{"location":"guides/loss-function/#temperature-scaling","title":"Temperature Scaling","text":"<p>Temperature controls the \"sharpness\" of the probability distribution:</p> <ul> <li>Lower temperature \u2192 More peaked distribution (more confident)</li> <li>Higher temperature \u2192 Flatter distribution (less confident)</li> </ul> Component Temperature Effect Student \u03c4=0.1 Sharper, more confident Teacher \u03c4=0.04 Even sharper, provides strong guidance <pre><code># Student: temperature = 0.1\nstudent_probs = softmax(logits / 0.1)\n\n# Teacher: temperature = 0.04 (sharper)\nteacher_probs = softmax(logits / 0.04)\n</code></pre>"},{"location":"guides/loss-function/#centering","title":"Centering","text":"<p>Centering prevents collapse (all predictions being the same):</p> <pre><code># Center is running mean of teacher outputs\ncenter = momentum * center + (1 - momentum) * teacher_outputs.mean()\n\n# Subtract center before temperature scaling\nteacher_logits = (teacher_outputs - center) / teacher_temp\n</code></pre> <p>Why centering?</p> <ul> <li>Without centering, the model can collapse to predicting the same thing for all inputs</li> <li>Centering ensures the teacher outputs are balanced around zero</li> <li>EMA update provides smooth, stable centering</li> </ul>"},{"location":"guides/loss-function/#asymmetry","title":"Asymmetry","text":"<p>The key asymmetry in DINO:</p> <ul> <li>Teacher: Only sees global views (2 large crops)</li> <li>Student: Sees all views (2 global + 6 local crops)</li> </ul> <p>This forces the student to learn features that work across scales and augmentations.</p>"},{"location":"guides/loss-function/#using-dinoloss","title":"Using DinoLoss","text":""},{"location":"guides/loss-function/#from-config-recommended","title":"From Config (Recommended)","text":"<pre><code>from dino.loss import DinoLoss\n\nloss_fn = DinoLoss.from_config(\n    config.loss,\n    config.augmentation,\n    out_dim=model.output_dim\n)\n</code></pre>"},{"location":"guides/loss-function/#manual-creation","title":"Manual Creation","text":"<pre><code>loss_fn = DinoLoss(\n    out_dim=2048,\n    student_temp=0.1,\n    teacher_temp=0.04,\n    center_momentum=0.9,\n    ncrops=8,\n    n_global_crops=2\n)\n</code></pre>"},{"location":"guides/loss-function/#forward-pass","title":"Forward Pass","text":"<pre><code># During training\nstudent_output = torch.cat([student(v) for v in all_views])\nteacher_output = torch.cat([teacher(v) for v in global_views])\n\nloss = loss_fn(student_output, teacher_output)\nloss.backward()\n</code></pre>"},{"location":"guides/loss-function/#configuration","title":"Configuration","text":"<pre><code>loss:\n  student_temp: 0.1\n  teacher_temp: 0.04\n  center_momentum: 0.9\n\naugmentation:\n  n_global_crops: 2\n  num_local_views: 6\n</code></pre>"},{"location":"guides/loss-function/#important-the-bug-fix","title":"Important: The Bug Fix","text":"<p>The original notebook had a bug that caused negative loss values. The issue was iterating over raw tensors instead of chunked probability distributions.</p> <p>Wrong (original): <pre><code>for i, teacher_output in enumerate(teacher_outputs):  # Raw tensors!\n    for j, student_output in enumerate(student_outputs):\n        loss = -torch.sum(teacher_output * student_output)\n</code></pre></p> <p>Correct (fixed): <pre><code># Chunk outputs into views\nstudent_views = student_log_probs.chunk(ncrops)\nteacher_views = teacher_probs.chunk(n_global_crops)\n\nfor i, teacher_prob in enumerate(teacher_views):  # Chunked distributions\n    for j, student_log_prob in enumerate(student_views):\n        loss = -torch.sum(teacher_prob * student_log_prob)\n</code></pre></p> <p>This implementation includes the fix - loss values should always be positive.</p>"},{"location":"guides/loss-function/#mathematical-details","title":"Mathematical Details","text":""},{"location":"guides/loss-function/#cross-entropy","title":"Cross-Entropy","text":"<p>For each (teacher view, student view) pair:</p> \\[L_{i,j} = -\\sum_k P_t^{(i)}(k) \\log P_s^{(j)}(k)\\] <p>Where: - \\(P_t^{(i)}\\) = teacher probability distribution for view \\(i\\) - \\(P_s^{(j)}\\) = student probability distribution for view \\(j\\)</p>"},{"location":"guides/loss-function/#total-loss","title":"Total Loss","text":"<p>Sum over all valid pairs (excluding same view):</p> \\[L = \\frac{1}{N} \\sum_{i \\in \\{global\\}} \\sum_{j \\in \\{all\\}, j \\neq i} L_{i,j}\\] <p>Where \\(N\\) is the number of valid pairs.</p>"},{"location":"guides/loss-function/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/loss-function/#loss-is-nan","title":"Loss is NaN","text":"<ul> <li>Check learning rate (try reducing it)</li> <li>Verify temperature values are positive</li> <li>Check for extreme values in outputs</li> </ul>"},{"location":"guides/loss-function/#loss-is-negative","title":"Loss is Negative","text":"<p>If you see negative loss values, something is wrong with the loss computation. This implementation should always produce positive loss values.</p>"},{"location":"guides/loss-function/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<ul> <li>Check batch size (need sufficient examples for meaningful statistics)</li> <li>Verify data augmentation is working</li> <li>Try adjusting temperature values</li> </ul>"},{"location":"guides/loss-function/#see-also","title":"See Also","text":"<ul> <li>Training - Training loop and optimization</li> <li>Models - Network architectures</li> <li>API Reference: Loss - API documentation</li> </ul>"},{"location":"guides/models/","title":"Models","text":"<p>This guide covers the neural network architectures used in DINO.</p>"},{"location":"guides/models/#architecture-overview","title":"Architecture Overview","text":"<p>DINO uses a student-teacher setup where both networks share the same architecture:</p> <pre><code>DinoModel = Backbone + Projection Head\n</code></pre> <pre><code>Input Image\n    \u2193\nBackbone (ResNet/ViT)\n    \u2193\nFeature Vector [batch, embed_dim]\n    \u2193\nProjection Head (MLP)\n    \u2193\nOutput [batch, output_dim]\n</code></pre>"},{"location":"guides/models/#backbones","title":"Backbones","text":""},{"location":"guides/models/#available-backbones","title":"Available Backbones","text":"Backbone Source Output Dim Parameters <code>resnet18</code> torchvision 512 11M <code>resnet34</code> torchvision 512 21M <code>resnet50</code> torchvision 2048 23M <code>resnet101</code> torchvision 2048 42M <code>resnet152</code> torchvision 2048 58M <code>dino_vits8</code> HuggingFace 384 21M <code>dino_vits16</code> HuggingFace 384 21M <code>dino_vitb8</code> HuggingFace 768 85M <code>dino_vitb16</code> HuggingFace 768 85M"},{"location":"guides/models/#using-backbones","title":"Using Backbones","text":"<pre><code>from dino.models import get_backbone\n\n# ResNet backbone\nbackbone = get_backbone('resnet18', pretrained=True)\nprint(backbone.output_dim)  # 512\n\n# ViT backbone (from HuggingFace)\nbackbone = get_backbone('dino_vits16', pretrained=True)\nprint(backbone.output_dim)  # 384\n</code></pre>"},{"location":"guides/models/#resnet-backbones","title":"ResNet Backbones","text":"<p>ResNet backbones use torchvision models with the final FC layer removed:</p> <pre><code>class ResnetBackboneDino(BackboneBase):\n    def __init__(self, variant='resnet18', pretrained=False):\n        self.model = resnet_variant(pretrained=pretrained)\n        self.model.fc = nn.Identity()  # Remove classifier\n        self.output_dim = 512  # or 2048 for ResNet50+\n</code></pre>"},{"location":"guides/models/#vit-backbones","title":"ViT Backbones","text":"<p>ViT backbones use pre-trained DINO models from HuggingFace:</p> <pre><code>class DinoBackbone(BackboneBase):\n    VARIANT_MAP = {\n        \"dino_vits8\": \"facebook/dino-vits8\",\n        \"dino_vits16\": \"facebook/dino-vits16\",\n        \"dino_vitb8\": \"facebook/dino-vitb8\",\n        \"dino_vitb16\": \"facebook/dino-vitb16\",\n    }\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        outputs = self.model(x, interpolate_pos_encoding=True)\n        return outputs.last_hidden_state[:, 0]  # CLS token\n</code></pre>"},{"location":"guides/models/#projection-head","title":"Projection Head","text":"<p>The projection head is a backbone-agnostic MLP that projects features to the output space.</p>"},{"location":"guides/models/#architecture","title":"Architecture","text":"<pre><code>Input [embed_dim]\n    \u2193\nLinear(embed_dim \u2192 hidden_dim) + GELU\n    \u2193\nLinear(hidden_dim \u2192 hidden_dim) + GELU\n    \u2193\nLinear(hidden_dim \u2192 bottleneck_dim)  # Bottleneck\n    \u2193\nL2 Normalize\n    \u2193\nWeight-Normalized Linear(bottleneck_dim \u2192 output_dim)\n    \u2193\nOutput [output_dim]\n</code></pre>"},{"location":"guides/models/#configuration","title":"Configuration","text":"<pre><code>model:\n  projection_hidden_dim: 1024\n  projection_bottleneck_dim: 256\n  projection_output_dim: 2048\n  projection_use_bn: false\n</code></pre>"},{"location":"guides/models/#using-projection-head","title":"Using Projection Head","text":"<pre><code>from dino.models import DinoProjectionHead\n\n# Manual creation\nprojection = DinoProjectionHead(\n    input_dim=512,      # From backbone\n    hidden_dim=1024,\n    bottleneck_dim=256,\n    output_dim=2048,\n    use_bn=False\n)\n\n# From config\nprojection = DinoProjectionHead.from_config(config.model, input_dim=512)\n</code></pre>"},{"location":"guides/models/#why-weight-normalization","title":"Why Weight Normalization?","text":"<ul> <li>Stabilizes training by constraining weight scale</li> <li>Prevents gradient explosion in final layer</li> <li>Used in the original DINO paper</li> </ul>"},{"location":"guides/models/#complete-dino-model","title":"Complete DINO Model","text":""},{"location":"guides/models/#creating-a-model","title":"Creating a Model","text":"<pre><code>from dino.models import DinoModel\n\n# Using factory method (recommended)\nmodel = DinoModel.from_config(config)\n\n# Manual creation\nfrom dino.models import get_backbone, DinoProjectionHead\n\nbackbone = get_backbone('resnet18')\nprojection = DinoProjectionHead(input_dim=512, output_dim=2048)\nmodel = DinoModel(backbone, projection)\n</code></pre>"},{"location":"guides/models/#forward-pass","title":"Forward Pass","text":"<pre><code># Standard forward\nprojections = model(images)  # [batch, output_dim]\n\n# Get backbone features (for evaluation)\nfeatures, projections = model(images, return_backbone_features=True)\n</code></pre>"},{"location":"guides/models/#model-properties","title":"Model Properties","text":"<pre><code>model = DinoModel.from_config(config)\nprint(model.output_dim)  # 2048\nprint(model.backbone.output_dim)  # 512 (for ResNet18)\n</code></pre>"},{"location":"guides/models/#student-teacher-setup","title":"Student-Teacher Setup","text":"<p>In DINO, two identical models are used:</p> <pre><code># Create student and teacher\nstudent = DinoModel.from_config(config)\nteacher = DinoModel.from_config(config)\n\n# Initialize teacher with student weights\nteacher.load_state_dict(student.state_dict())\n\n# Disable gradients for teacher (updated via EMA)\nfor param in teacher.parameters():\n    param.requires_grad = False\n</code></pre> <p>The teacher is updated using Exponential Moving Average (EMA):</p> <pre><code>from dino.utils import update_teacher_EMA\n\n# After each training step\nupdate_teacher_EMA(student, teacher, momentum=0.996)\n</code></pre> <p>See Training for details on the training loop.</p>"},{"location":"guides/models/#model-configuration","title":"Model Configuration","text":"<pre><code>model:\n  # Backbone\n  backbone: resnet18           # or dino_vits16, resnet50, etc.\n  pretrained_backbone: false   # Use pretrained weights\n\n  # Projection head\n  projection_hidden_dim: 1024\n  projection_bottleneck_dim: 256\n  projection_output_dim: 2048\n  projection_use_bn: false\n</code></pre>"},{"location":"guides/models/#adding-custom-backbones","title":"Adding Custom Backbones","text":"<ol> <li>Create backbone class in <code>src/dino/models/backbone/</code>:</li> </ol> <pre><code># src/dino/models/backbone/efficientnet.py\nfrom .backbone import BackboneBase\nimport timm\n\nclass EfficientNetBackbone(BackboneBase):\n    def __init__(self, variant='efficientnet_b0', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(variant, pretrained=pretrained, num_classes=0)\n        self.output_dim = self.model.num_features\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre> <ol> <li>Update factory function in <code>src/dino/models/backbone/backbone.py</code>:</li> </ol> <pre><code>def get_backbone(name, pretrained=False):\n    # ... existing code ...\n    elif name.startswith('efficientnet'):\n        from .efficientnet import EfficientNetBackbone\n        return EfficientNetBackbone(name, pretrained)\n</code></pre> <ol> <li>Export in <code>__init__.py</code> and use in config!</li> </ol>"},{"location":"guides/models/#see-also","title":"See Also","text":"<ul> <li>Training - Training loop and EMA</li> <li>Loss Function - DINO loss computation</li> <li>API Reference: Models - API documentation</li> </ul>"},{"location":"guides/training/","title":"Training","text":"<p>This guide explains the DINO training pipeline, including the training loop, EMA updates, and optimizers.</p>"},{"location":"guides/training/#overview","title":"Overview","text":"<p>The DINO training pipeline:</p> <ol> <li>Forward pass: Student sees all views, teacher sees global views only</li> <li>Loss computation: Cross-entropy between student and teacher predictions</li> <li>Backward pass: Gradients only through student</li> <li>EMA update: Teacher weights updated from student via exponential moving average</li> </ol>"},{"location":"guides/training/#training-loop","title":"Training Loop","text":""},{"location":"guides/training/#high-level-flow","title":"High-Level Flow","text":"<pre><code>for epoch in range(num_epochs):\n    for batch_idx, (views, _) in enumerate(train_loader):\n        # 1. Separate views\n        global_views = views[:2]     # First 2 are global\n        all_views = views            # All 8 views\n\n        # 2. Teacher forward (no gradients)\n        with torch.no_grad():\n            teacher_output = concat([teacher(v) for v in global_views])\n\n        # 3. Student forward (with gradients)\n        student_output = concat([student(v) for v in all_views])\n\n        # 4. Compute loss\n        loss = dino_loss(student_output, teacher_output)\n\n        # 5. Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        clip_grad_norm_(student.parameters(), max_norm)  # Optional\n        optimizer.step()\n\n        # 6. Learning rate scheduler step\n        scheduler.step()\n\n        # 7. EMA update for teacher\n        update_teacher_EMA(student, teacher, momentum)\n</code></pre>"},{"location":"guides/training/#using-dinotrainer","title":"Using DinoTrainer","text":"<pre><code>from dino.training import DinoTrainer, create_optimizer, create_scheduler\nfrom dino.models import DinoModel\nfrom dino.loss import DinoLoss\nfrom dino.data import create_dataloaders\n\n# Setup\nconfig = DinoConfig.from_yaml('config.yaml')\ntrain_loader, val_loader, _ = create_dataloaders(config)\n\nstudent = DinoModel.from_config(config)\nteacher = DinoModel.from_config(config)\nteacher.load_state_dict(student.state_dict())\n\nloss_fn = DinoLoss.from_config(config.loss, config.augmentation, student.output_dim)\noptimizer = create_optimizer(student.parameters(), config.optimizer)\nscheduler = create_scheduler(optimizer, config.scheduler, config.optimizer, total_steps, warmup_steps)\n\n# Create trainer\ntrainer = DinoTrainer(\n    config=config,\n    student=student,\n    teacher=teacher,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loss_fn=loss_fn,\n    train_loader=train_loader\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"guides/training/#ema-exponential-moving-average","title":"EMA (Exponential Moving Average)","text":""},{"location":"guides/training/#how-ema-works","title":"How EMA Works","text":"<p>The teacher is updated as a moving average of the student:</p> <pre><code>teacher = momentum * teacher + (1 - momentum) * student\n</code></pre> <pre><code>@torch.no_grad()\ndef update_teacher_EMA(student, teacher, momentum=0.996):\n    for s_param, t_param in zip(student.parameters(), teacher.parameters()):\n        t_param.data = momentum * t_param.data + (1 - momentum) * s_param.data\n</code></pre>"},{"location":"guides/training/#momentum-scheduling","title":"Momentum Scheduling","text":"<p>The momentum typically increases during training:</p> <ul> <li>Early training: Lower momentum (0.996) - teacher learns faster</li> <li>Late training: Higher momentum (\u21921.0) - teacher becomes stable</li> </ul> <pre><code>from dino.utils import get_momentum_schedule\n\nschedule = get_momentum_schedule(\n    base=0.996,\n    final=1.0,\n    num_epochs=100,\n    niter_per_epoch=len(train_loader)\n)\n\n# During training\nmomentum = schedule[current_iteration]\nupdate_teacher_EMA(student, teacher, momentum)\n</code></pre>"},{"location":"guides/training/#configuration","title":"Configuration","text":"<pre><code>training:\n  teacher_momentum: 0.996\n  teacher_momentum_final: 1.0\n  use_momentum_schedule: true\n</code></pre>"},{"location":"guides/training/#optimizer","title":"Optimizer","text":""},{"location":"guides/training/#supported-optimizers","title":"Supported Optimizers","text":"<p>Currently supports AdamW:</p> <pre><code>from dino.training import create_optimizer\n\noptimizer = create_optimizer(\n    model.parameters(),\n    config.optimizer\n)\n</code></pre>"},{"location":"guides/training/#configuration_1","title":"Configuration","text":"<pre><code>optimizer:\n  optimizer: adamw\n  lr: 0.001\n  weight_decay: 0.04\n  betas: [0.9, 0.999]\n</code></pre>"},{"location":"guides/training/#learning-rate-scheduler","title":"Learning Rate Scheduler","text":""},{"location":"guides/training/#cosine-with-warmup","title":"Cosine with Warmup","text":"<p>The default scheduler uses linear warmup followed by cosine decay:</p> <pre><code>LR\n\u2502\n\u2502  \u2571\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u2572\n\u2502 \u2571                \u2572\n\u2502\u2571                  \u2572___\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Steps\n  \u2502warmup\u2502   cosine decay\n</code></pre> <pre><code>from dino.training import create_scheduler\n\nscheduler = create_scheduler(\n    optimizer,\n    config.scheduler,\n    config.optimizer,\n    total_steps=num_epochs * len(train_loader),\n    warmup_steps=warmup_epochs * len(train_loader)\n)\n</code></pre>"},{"location":"guides/training/#configuration_2","title":"Configuration","text":"<pre><code>scheduler:\n  scheduler: cosine_warmup\n  warmup_epochs: 10\n  min_lr: 1e-6\n  warmup_start_lr: 0.0\n</code></pre>"},{"location":"guides/training/#how-it-works","title":"How It Works","text":"<ol> <li>Warmup phase: Linear increase from <code>warmup_start_lr</code> to <code>optimizer.lr</code></li> <li>Cosine phase: Cosine decay from <code>optimizer.lr</code> to <code>min_lr</code></li> </ol>"},{"location":"guides/training/#gradient-clipping","title":"Gradient Clipping","text":"<p>Prevents exploding gradients during training:</p> <pre><code>if config.training.gradient_clip:\n    torch.nn.utils.clip_grad_norm_(\n        student.parameters(),\n        config.training.gradient_clip\n    )\n</code></pre>"},{"location":"guides/training/#configuration_3","title":"Configuration","text":"<pre><code>training:\n  gradient_clip: 3.0  # Max gradient norm\n</code></pre>"},{"location":"guides/training/#training-configuration","title":"Training Configuration","text":"<pre><code>training:\n  num_epochs: 100\n  teacher_momentum: 0.996\n  teacher_momentum_final: 1.0\n  use_momentum_schedule: true\n  gradient_clip: 3.0\n\noptimizer:\n  optimizer: adamw\n  lr: 0.001\n  weight_decay: 0.04\n\nscheduler:\n  scheduler: cosine_warmup\n  warmup_epochs: 10\n  min_lr: 1e-6\n</code></pre>"},{"location":"guides/training/#training-history","title":"Training History","text":"<p>The <code>History</code> class tracks metrics during training:</p> <pre><code>from dino.utils import History\n\nhistory = History()\n\n# Record during training\nhistory.record_iteration(iteration, {\n    'loss': loss.item(),\n    'learning_rate': optimizer.param_groups[0]['lr'],\n    'momentum': current_momentum\n})\n\nhistory.record_epoch(epoch, {\n    'loss': epoch_loss,\n    'learning_rate': current_lr,\n    'momentum': current_momentum\n})\n\n# Save/Load\nhistory.save('training_history.json')\nhistory = History.load('training_history.json')\n\n# Visualization\nhistory.plot_all(level='epoch', save_path='plots.png')\n</code></pre>"},{"location":"guides/training/#initialization-sequence","title":"Initialization Sequence","text":"<p>The complete initialization flow:</p> <ol> <li>Configuration: Load YAML \u2192 Override with CLI args</li> <li>Logging: Setup file and console handlers</li> <li>Data: Create dataloaders with <code>create_dataloaders(config)</code></li> <li>Models: Create with <code>DinoModel.from_config(config)</code></li> <li>Teacher: Copy student weights \u2192 Disable gradients</li> <li>Loss: Create with <code>DinoLoss.from_config(config.loss, config.augmentation, out_dim)</code></li> <li>Optimizer: Create with <code>create_optimizer(params, config.optimizer)</code></li> <li>Scheduler: Create with <code>create_scheduler(...)</code></li> <li>Trainer: Assemble all components</li> <li>Resume (optional): Load checkpoint</li> <li>Train: Run training loop</li> </ol>"},{"location":"guides/training/#see-also","title":"See Also","text":"<ul> <li>Loss Function - DINO loss computation</li> <li>Checkpointing - Saving and resuming training</li> <li>Configuration - Full configuration reference</li> <li>Performance - Optimization tips</li> </ul>"}]}